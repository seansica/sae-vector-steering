@misc{li2024inferencetimeintervention,
  title         = {Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author        = {Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  year          = {2024},
  eprint        = {2306.03341},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{lecomte2024causespolysemanticity,
  title         = {What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes},
  author        = {Lecomte, Victor and Thaman, Kushal and Schaeffer, Rylan and Bashkansky, Naomi and Chow, Trevor and Koyejo, Sanmi},
  year          = {2024},
  eprint        = {2312.03096},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{cunningham2023sparseautoencoders,
  title         = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author        = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year          = {2023},
  eprint        = {2309.08600},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{scherlis2023polysemanticity,
  title         = {Polysemanticity and Capacity in Neural Networks},
  author        = {Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S. and Benton, Joe and Shlegeris, Buck},
  year          = {2023},
  eprint        = {2210.01892},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}

@misc{bills2023language,
  title        = {Language models can explain neurons in language models},
  author       = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  year         = {2023},
  howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
}

@article{bricken2023monosemanticity,
  title   = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author  = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
  year    = {2023},
  journal = {Transformer Circuits Thread},
  note    = {\url{https://transformer-circuits.pub/2023/monosemantic-features/index.html}}
}

@article{templeton2024scaling,
  title   = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
  author  = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
  year    = {2024},
  journal = {Transformer Circuits Thread},
  url     = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{elhage2021mathematical,
  title   = {A Mathematical Framework for Transformer Circuits},
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year    = {2021},
  journal = {Transformer Circuits Thread},
  note    = {\url{https://transformer-circuits.pub/2021/framework/index.html}}
}

@article{elhage2022superposition,
  title   = {Toy Models of Superposition},
  author  = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year    = {2022},
  journal = {Transformer Circuits Thread},
  note    = {\url{https://transformer-circuits.pub/2022/toy_model/index.html}}
}