\section{Background}

Our research builds upon Anthropic's work on superposition and polysemanticity in neural networks \cite{bricken2023monosemanticity, templeton2024scaling, elhage2021mathematical, elhage2022superposition}. Their studies show that neurons in neural networks are polysemantic, responding to mixtures of seemingly unrelated inputs due to superposition, where a network represents more independent "features" than it has neurons. This phenomenon is analogous to widespread brain activation in response to specific stimuli during neurological studies. Scaling the neural network to have more neurons than features is not viable due to incidental polysemanticity, which Lecomte et al. \cite{lecomte2024causespolysemanticity} suggest can manifest from factors like regularization and neural noise, proposing that adjusting the learning trajectory might mitigate this issue without altering the neural architecture.

To address this, SAEs have emerged as a powerful tool. SAEs are neural networks designed to learn sparse representations of input data, typically with more hidden units than input units. By enforcing sparsity in the hidden layer, SAEs encourage the network to learn more interpretable and disentangled features, akin to sparse dictionary learning \cite{cunningham2023sparseautoencoders}. The encoder detects features by finding optimal directions in the activation space, while the decoder represents these features, approximating their "true" directions regardless of interference. This division allows SAEs to potentially recover individual features superposed in the original network \cite{bricken2023monosemanticity}.

Researchers at OpenAI have proposed LLM-driven solutions using prompt engineering to interpret individual neurons \cite{bills2023language}. While we are skeptical about its practicality and reliability, this method inspires our research. We aim to leverage prompt engineering to analyze and label sparse features from sparse autoencoders. By synthesizing Anthropic's approach of decomposing neuron activations into sparse, interpretable features with OpenAI's neuron-centric interpretability solution, we seek to enhance the interpretability of fine-tuned language models and provide a robust framework for understanding their internal mechanisms. We verify our learned features' semantic significance through coherence scores, intervention tests, and human evaluation exercises, collectively providing strong evidence for their interpretability.

