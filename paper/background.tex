\section{Background}

The foundation of our research lies in Anthropic's work on superposition, polysemanticity, and toy models in neural networks \cite{bricken2023monosemanticity, templeton2024scaling, elhage2021mathematical, elhage2022superposition}. Their research asserts that neurons in neural networks are polysemantic, responding to mixtures of seemingly unrelated inputs. When inputs are fed into a transformer model, a substantial subset of neurons typically show non-negligible activation. This phenomenon is analogous to observing widespread brain activation in response to specific stimuli during neurological studies.

Anthropic hypothesizes that this behavior results from superposition, where a neural network represents more independent "features" of the data than it has neurons by assigning each feature its own linear combination of neurons. This suggests that we may be able to recover the network's features by finding a set of directions in activation space such that each activation vector can be reconstructed from a sparse linear combination of these directions. This approach is equivalent to the well-known problem of sparse dictionary learning \cite{cunningham2023sparseautoencoders}. Through experiments with toy models, Anthropic demonstrated that sparse dictionary learning can be employed to causally reduce polysemanticity, making inference activations far more sparse and enabling more effective causal inference for neuron labeling.

Scaling the neural network to have more neurons than features is not a viable solution due to incidental polysemanticity. Lecomte et al. \cite{lecomte2024causespolysemanticity} suggest that polysemanticity can manifest for multiple reasons, including regularization and neural noise. They propose that incidental polysemanticity might be mitigated by adjusting the learning trajectory without necessarily altering the neural architecture.

In a different approach, researchers at OpenAI have proposed LLM-driven solutions to tackle the interpretability issue \cite{bills2023language}. Their method uses prompt engineering to interpret individual neurons. While we approach this method with some skepticism regarding its practicality and reliability, it provides inspiration for our research. We aim to leverage prompt engineering to aid in analyzing and labeling sparse features extracted from sparse autoencoders.

Sparse autoencoders (SAEs) have emerged as a powerful tool for addressing the challenges of superposition and polysemanticity in neural networks. An SAE is a type of neural network designed to learn a sparse representation of input data, typically with more hidden units than input units. The key insight behind SAEs is that by enforcing sparsity in the hidden layer, we can encourage the network to learn more interpretable and disentangled features. Conceptually, the encoder component of an SAE detects features by finding optimal directions in the activation space to project onto, minimizing interference with other similar features. The decoder, on the other hand, attempts to represent these features, approximating their "true" directions regardless of interference. This division of labor allows SAEs to potentially recover individual features that are superposed in the original network. By applying an L1 regularization penalty during training, SAEs are incentivized to activate only a small subset of hidden units for any given input, leading to more focused and interpretable representations. This approach has shown promise in decomposing the complex, polysemantic representations found in large language models into more semantically coherent and manipulable features. \cite{bricken2023monosemanticity}

Our research seeks to synthesize Anthropic's approach of decomposing neuron activations into sparse, interpretable features with OpenAI's neuron-centric automated interpretability solution. By combining these methodologies, we aim to enhance the interpretability of fine-tuned language models and provide a more robust framework for understanding their internal mechanisms.

Our research seeks to synthesize Anthropic's approach of decomposing neuron activations into sparse, interpretable features with OpenAI's neuron-centric automated interpretability solution. By combining these methodologies, we aim to enhance the interpretability of fine-tuned language models and provide a more robust framework for understanding their internal mechanisms.

To verify that our learned features represent a semantically meaningful decomposition of the activation space, we employ several techniques. First, we demonstrate that our features are, on average, more interpretable than neurons and other matrix decomposition techniques, as measured by coherence scores. We then confirm these findings through both a causal intervention "smoke test" and a human evaluation. These methods collectively provide strong evidence for the semantic significance of our extracted features.