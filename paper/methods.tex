\section{Methods}

% Size: 2 - 3 pages
% Focus: design and implementation
% Questions to answer: 
% What is your proposed approach?
% Describe your problem using an example or two.
% What is the intuition behind your proposed approach?
% How are you solving the problem?
% What are your experiments/experimental design?
% Why are the experiments helping to solve your problem?
% How are you measuring success?


We designed an experiment involving a control and treatment model to isolate the effects of fine-tuning on a language model's internal representations. The treatment effect is the application of fine-tuned data to the control/baseline model. Our aim is to isolate the causal effects of fine-tuning on the resultant features extracted from the model's sparse autoencoder, allowing us to observe how features change with respect to model training. Specifically, we extract features and analyze their sharpness using an empirical coherence score and verify their supposed identities using a rigorous intervention test.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our experimental approach is designed to use causal inference to directly observe the impact of the fine-tuning process on the model's learned features while holding all other variables constant. The key components of our experimental design are:

\begin{enumerate}
    \item Control: A baseline GPT-2 model with a pretrained SAE.

    \item Treatment: A GPT-2 model fined-tuned on a dataset containing over 6,000 medical terms and their Wikipedia text with a custom-trained SAE.\hyperlink{med-gpt2}{[1]}

    \item Feature Extraction: A process to extract interpretable features from both SAEs.

    \item Feature Analysis: Methods to assess feature sharpness and verify feature identities.
\end{enumerate}

The intervention experiment is a key component of our methodology. It involves applying a steering vector to the transformer using a hook point positioned at the "pre" stage of the residual stream. This steering vector allows us to control the activation strength of a candidate feature using a coefficient multiplier and a temperature value (ranging from 0 to 1.0).

For each feature that passes our coherence and human evaluation criteria, we conduct an experiment wherein next token prediction/inference is triggered in response to a relevant prompt. We determine the feature to be a match (i.e., the feature is what we suppose it to be based on its top activating tokens) if the steering vector causes the model output to drastically shift in the direction of the candidate feature label. For example, if we identify a feature that appears to represent (or "fire for") words like "doctor" and "nurse", we would expect a steering vector applied to this feature to overrepresent those words in its next token predictions. This approach allows us to verify the semantic meaning of the extracted features and assess how fine-tuning affects the model's internal representations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Baseline Model and Pretrained SAE}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The control consists of our baseline model, gpt2-small, with a pretrained SAE model\hyperlink{baseline-details}{[2]} trained on the token activations extracted from the residual stream of the transformer model. Key specifications of the baseline model are provided in the Appendix\hyperlink{gpt2-specs}{[3]}.

The SAE consists of three layers: an encoder, multi-layer perceptron (MLP), and decoder. It accepts batches of internal activations from the residual stream of our transformer model. Notably, we apply a bias on the input layer for performance purposes, as suggested by Bricken et al. \cite{bricken2023monosemanticity}. The SAE is trained using the Adam optimizer to reconstruct the MLP activations of the gpt2-small model, with an MSE loss and an L1 penalty to encourage feature sparsity.

This architecture implies a significant expansion in the feature space, with the SAE operating on a space more than 32 times larger than the original model's hidden state ($24576 / 768 \approx 32$). This expansion allows the SAE to potentially capture and isolate a much richer set of features than are directly represented in the original model's activations.

The SAE was trained on activations extracted from the eighth attention block of the transformer. Specifically, it processes the activations from the residual stream. The training data was sourced from the "Skylion007/openwebtext" dataset, and no normalization was applied to the activations during training.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fine-tuned Model and Custom SAE}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our treatment consisted of a fine-tuned variant of the control model. The transformer architecture remains the same gpt2-small model, but it is fine-tuned on a specialized dataset related to Medical Wiki Text. The sparse autoencoder for this group was custom trained on the activation vectors extracted from the transformer's residual stream, mirroring the approach used in the control model. 

The training was orchestrated using a carefully configured SAE training pipeline.\hyperlink{sae-training}{[3]} Several architectural choices were made to optimize the SAE's performance: (1) The decoder bias was initialized to zeros. (2) The decoder was not normalized, but the sparsity penalty was scaled by the decoder norm. (3) A heuristic initialization was used for the decoder, and the encoder was initially set as the transpose of the decoder. (4) Input activations were normalized using an "expected average only in" approach.


To address the potential issue of dead features, the training process included a resampling protocol. This protocol monitored feature activations over a window of 1000 steps, identifying and potentially resampling features that remained inactive (activation below 1e-4) throughout this period.

By using this custom-trained SAE on our fine-tuned GPT-2 model, we aim to capture and isolate features that are specifically relevant to the patient transcript domain, potentially revealing insights into how the fine-tuning process affects the model's internal representations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature Extraction and Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To extract interpretable features from our sparse autoencoders, we employ a multi-step process:

\begin{enumerate}
    \item Projection of SAE Decoder Weights: We project the SAE's decoder weights ($W_{dec}$) onto the unembedding matrix ($W_{U}$) of the transformer model. This projection is computed as:
    
        \[ Decoder\ Projection\ On\ W_{U} = W_{dec} \otimes W_{U} \]
    
        where $\otimes$ denotes matrix multiplication.
    
    \item Feature-to-Vocabulary Mapping: This projection allows us to map the SAE's learned features directly onto the model's vocabulary space, effectively representing how each SAE feature influences the prediction of each token in the model's vocabulary.
    
    \item Top Activated Words Identification: For each feature, we identify the top activated words based on the projection. This helps us infer the semantic meaning or function of each SAE feature in the context of the language model's vocabulary and task.

    \item Coherence Score: For each feature, we use a pre-trained word embedding model (GloVe) to calculate the average cosine similarity between all pairs of top activating tokens for each feature. A higher average similarity indicates greater semantic coherence.

    % WE HAVE NOT ACTUALLY DONE THIS YETâ€“LEAVE OUT UNLESS/UNTIL COMPLETED:
    % \item Sparsity Analysis: We incorporate sparsity measurements of the SAE features into our analysis, allowing us to assess how focused or distributed each feature's influence is across the model's vocabulary space.
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measuring Success}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We employ several methods to measure the success of our approach:

\begin{enumerate}
    \item \textit{Empirical Coherence Score}: Analyzes the sharpness of extracted features.
    \item \textit{Causal Intervention Test}: Verifies the semantic meaning of extracted features as described in the Experimental Design section.
    \item \textit{Human Evaluation}: Uses keyword matching to identify "medical" features (at least two medical keyword matches) followed by thorough human verification to ensure semantic coherence and avoid false positives (such as synonym/subword misinterpretation).
    \item \textit{Comparative Analysis}: Compares features from the baseline model and the fine-tuned model to assess the impact of fine-tuning on feature interpretability and relevance.
\end{enumerate}

This comprehensive approach allows us to extract meaningful features from transformer models and assess the impact of fine-tuning quantitatively and qualitatively. Combining projection-based feature interpretation and rigorous testing provides a robust framework for understanding how fine-tuning affects a language model's internal representations.
