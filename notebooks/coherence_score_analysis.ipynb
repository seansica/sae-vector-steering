{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Optional, Callable, Union, List, Tuple\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "from sae_lens import SAE\n",
    "import plotly.express as px\n",
    "\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import (\n",
    "    load_dataset,\n",
    "    tokenize_and_concatenate,\n",
    "    download_file_from_hf,\n",
    ")\n",
    "# from plotly_utils import imshow, line, hist\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Score Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the pretrained transformer and SAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"gpt2-small-res-jb\", sae_id=\"blocks.8.hook_resid_pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next load the fine-tuned transformer and custom SAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "path_to_custom_transformer_dict = '../models/fine-tuned/fine_tuned_gpt2/model.safetensors'\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "# Initialize the HookedTransformer with the same architecture as your fine-tuned model\n",
    "custom_model = HookedTransformer.from_pretrained(\"gpt2\")  # base model\n",
    "\n",
    "# Load the state dict from the .safetensors file\n",
    "with safe_open(path_to_custom_transformer_dict, framework=\"pt\", device=device) as f:\n",
    "    state_dict = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# Load the state dict into the model\n",
    "custom_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "custom_model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_custom_sae_dict = '../models/sae/gpt2-small-fine-tuned-layer-8'\n",
    "custom_sae = SAE.load_from_pretrained(path_to_custom_sae_dict, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_stats(feature_summaries, feature_ids=None):\n",
    "    if feature_ids is None:\n",
    "        coherence_scores = [summary['coherence_score'] for summary in feature_summaries.values()]\n",
    "    else:\n",
    "        coherence_scores = [feature_summaries[i]['coherence_score'] for i in feature_ids if i in feature_summaries]\n",
    "    \n",
    "    coherence_scores = np.array(coherence_scores)\n",
    "    non_zero_scores = coherence_scores[coherence_scores > 0]\n",
    "    \n",
    "    stats = {\n",
    "        \"mean_all\": np.mean(coherence_scores),\n",
    "        \"median_all\": np.median(coherence_scores),\n",
    "        \"mean_non_zero\": np.mean(non_zero_scores) if len(non_zero_scores) > 0 else 0,\n",
    "        \"median_non_zero\": np.median(non_zero_scores) if len(non_zero_scores) > 0 else 0,\n",
    "        \"fraction_non_zero\": len(non_zero_scores) / len(coherence_scores),\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def semantic_coherence_score(activated_tokens, activation_scores, word_embeddings):\n",
    "    tokens_and_scores = [(t.lower(), score) for t, score in zip(activated_tokens, activation_scores) if t.lower().isalpha()]\n",
    "\n",
    "    token_embeddings = []\n",
    "    weights = []\n",
    "    for token, score in tokens_and_scores:\n",
    "        if token in word_embeddings:\n",
    "            token_embeddings.append(word_embeddings[token])\n",
    "            weights.append(score)\n",
    "\n",
    "    similarities = []\n",
    "    total_weight = 0\n",
    "    for i in range(len(token_embeddings)):\n",
    "        for j in range(i+1, len(token_embeddings)):\n",
    "            sim = 1 - cosine(token_embeddings[i], token_embeddings[j])\n",
    "            weight = weights[i] * weights[j]\n",
    "            similarities.append(sim * weight)\n",
    "            total_weight += weight\n",
    "\n",
    "    return np.sum(similarities) / total_weight if total_weight > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "GloVe embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load word embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "word_embeddings = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(\"GloVe embeddings loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.analysis.feature_statistics import get_W_U_W_dec_stats_df\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from torch import Tensor, topk\n",
    "\n",
    "def get_top_k_words(\n",
    "    feature_activations: Tensor, words: List[str], k: int = 10\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Get the top k activated words for a given feature.\n",
    "\n",
    "    Args:\n",
    "        feature_activations (torch.Tensor): Activation values for a feature.\n",
    "        words (List[str]): List of words in the vocabulary.\n",
    "        k (int): Number of top words to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: List of tuples containing top words and their activation values.\n",
    "    \"\"\"\n",
    "    if feature_activations.numel() == 0:\n",
    "        return []\n",
    "\n",
    "    k = min(k, feature_activations.numel())\n",
    "    top_k_values, top_k_indices = topk(feature_activations, k)\n",
    "    top_k_words = [words[i] for i in top_k_indices.tolist()]\n",
    "    top_k_activations = top_k_values.tolist()\n",
    "\n",
    "    return list(zip(top_k_words, top_k_activations))\n",
    "\n",
    "def get_feature_summaries(sae, word_embeddings):\n",
    "    # Assuming W_dec, model, and other necessary variables are already defined\n",
    "    W_dec = sae.W_dec.detach().cpu()\n",
    "    W_U_stats_df_dec, dec_projection_onto_W_U = get_W_U_W_dec_stats_df(\n",
    "        W_dec, model, cosine_sim=False\n",
    "    )\n",
    "\n",
    "    number_of_features = dec_projection_onto_W_U.shape[0]\n",
    "\n",
    "    # Get vocabulary\n",
    "    vocab = model.tokenizer.get_vocab()\n",
    "    words = sorted(vocab.keys(), key=lambda x: vocab[x])\n",
    "\n",
    "    feature_summaries = {}\n",
    "    for i in tqdm(range(number_of_features), desc=\"Processing features\"):\n",
    "        feature_activations = dec_projection_onto_W_U[i]\n",
    "        top_activated_words = get_top_k_words(feature_activations, words)\n",
    "\n",
    "        activated_tokens, activation_scores = zip(*top_activated_words)\n",
    "\n",
    "        coherence_score = semantic_coherence_score(activated_tokens, activation_scores, word_embeddings)\n",
    "        feature_summary = {\n",
    "            \"feature_idx\": i,\n",
    "            \"top_activated_words\": top_activated_words,\n",
    "            \"activation_scores\": activation_scores,\n",
    "            \"coherence_score\": coherence_score\n",
    "        }\n",
    "        feature_summaries[i] = feature_summary\n",
    "    \n",
    "    return feature_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "Processing features:   0%|          | 0/24576 [00:00<?, ?it/s]To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing features: 100%|██████████| 24576/24576 [00:23<00:00, 1060.10it/s]\n",
      "Processing features: 100%|██████████| 24576/24576 [00:27<00:00, 906.09it/s] \n"
     ]
    }
   ],
   "source": [
    "baseline_feature_summaries = get_feature_summaries(sae, word_embeddings)\n",
    "finetuned_feature_summaries = get_feature_summaries(custom_sae, word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Feature IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "path_to_parsed_features = '../features/parsed_features.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(path_to_parsed_features, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to convert the nested dictionary to a DataFrame\n",
    "def dict_to_df(data_dict):\n",
    "    rows = []\n",
    "    for feature, tokens in data_dict.items():\n",
    "        for token, activation in tokens.items():\n",
    "            rows.append({\n",
    "                'feature': int(feature),\n",
    "                'token': token,\n",
    "                'activation': activation\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create DataFrames for baseline and finetuned data\n",
    "baseline_df = dict_to_df(data['baseline'])\n",
    "finetuned_df = dict_to_df(data['finetuned'])\n",
    "\n",
    "baseline_feature_ids = baseline_df.feature.unique()\n",
    "finetuned_feature_ids = finetuned_df.feature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline gpt2-small:\n",
      "All features:\n",
      "  Mean (all): 0.0719\n",
      "  Median (all): 0.0000\n",
      "  Mean (non-zero): 0.2057\n",
      "  Median (non-zero): 0.1399\n",
      "  Fraction of non-zero scores: 38.82%\n",
      "Medical features:\n",
      "  Mean (all): 0.0644\n",
      "  Median (all): 0.0000\n",
      "  Mean (non-zero): 0.1998\n",
      "  Median (non-zero): 0.1342\n",
      "  Fraction of non-zero scores: 37.62%\n",
      "\n",
      "Fine-tuned gpt2-small:\n",
      "All features:\n",
      "  Mean (all): 0.0728\n",
      "  Median (all): 0.0246\n",
      "  Mean (non-zero): 0.1470\n",
      "  Median (non-zero): 0.1026\n",
      "  Fraction of non-zero scores: 57.06%\n",
      "Medical features:\n",
      "  Mean (all): 0.0671\n",
      "  Median (all): 0.0000\n",
      "  Mean (non-zero): 0.1584\n",
      "  Median (non-zero): 0.0938\n",
      "  Fraction of non-zero scores: 47.12%\n"
     ]
    }
   ],
   "source": [
    "# Calculate stats for both models\n",
    "baseline_stats = calculate_coherence_stats(baseline_feature_summaries)\n",
    "baseline_medical_stats = calculate_coherence_stats(baseline_feature_summaries, baseline_feature_ids)\n",
    "finetuned_stats = calculate_coherence_stats(finetuned_feature_summaries)\n",
    "finetuned_medical_stats = calculate_coherence_stats(finetuned_feature_summaries, finetuned_feature_ids)\n",
    "\n",
    "# Function to print stats\n",
    "def print_stats(name, stats):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean (all): {stats['mean_all']:.4f}\")\n",
    "    print(f\"  Median (all): {stats['median_all']:.4f}\")\n",
    "    print(f\"  Mean (non-zero): {stats['mean_non_zero']:.4f}\")\n",
    "    print(f\"  Median (non-zero): {stats['median_non_zero']:.4f}\")\n",
    "    print(f\"  Fraction of non-zero scores: {stats['fraction_non_zero']:.2%}\")\n",
    "\n",
    "# Print stats for both models\n",
    "print(\"Baseline gpt2-small:\")\n",
    "print_stats(\"All features\", baseline_stats)\n",
    "print_stats(\"Medical features\", baseline_medical_stats)\n",
    "\n",
    "print(\"\\nFine-tuned gpt2-small:\")\n",
    "print_stats(\"All features\", finetuned_stats)\n",
    "print_stats(\"Medical features\", finetuned_medical_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
