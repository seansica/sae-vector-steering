{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Coherence and Vector Steering Analysis\n",
    "\n",
    "This notebook analyzes the relationship between feature coherence scores and vector steering outcomes for both baseline and fine-tuned GPT-2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torch import cosine_similarity\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim.downloader as api\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from sae_lens.analysis.feature_statistics import get_W_U_W_dec_stats_df\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the baseline model and pretrained SAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\").to(device)\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"gpt2-small-res-jb\", sae_id=\"blocks.8.hook_resid_pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fine-tuned model and custom SAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "path_to_custom_transformer_dict = '../models/fine-tuned/fine_tuned_gpt2/model.safetensors'\n",
    "\n",
    "# Initialize the HookedTransformer with the same architecture as your fine-tuned model\n",
    "custom_model = HookedTransformer.from_pretrained(\"gpt2\")  # base model\n",
    "\n",
    "# Load the state dict from the .safetensors file\n",
    "with safe_open(path_to_custom_transformer_dict, framework=\"pt\", device=device) as f:\n",
    "    state_dict = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# Load the state dict into the model\n",
    "custom_model.load_state_dict(state_dict, strict=False)\n",
    "custom_model = custom_model.to(device)\n",
    "\n",
    "# Load the custom SAE\n",
    "path_to_custom_sae_dict = '../models/sae/gpt2-small-fine-tuned-layer-8'\n",
    "custom_sae = SAE.load_from_pretrained(path_to_custom_sae_dict, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "GloVe embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load word embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "word_embeddings = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(\"GloVe embeddings loaded.\")\n",
    "\n",
    "# Load parsed features\n",
    "path_to_parsed_features = '../features/parsed_features.json'\n",
    "with open(path_to_parsed_features, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(data_dict):\n",
    "    rows = []\n",
    "    for feature, tokens in data_dict.items():\n",
    "        for token, activation in tokens.items():\n",
    "            rows.append({\n",
    "                'feature': int(feature),\n",
    "                'token': token,\n",
    "                'activation': activation\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_top_k_words(feature_activations: torch.Tensor, words: List[str], k: int = 10) -> List[Tuple[str, float]]:\n",
    "    if feature_activations.numel() == 0:\n",
    "        return []\n",
    "    k = min(k, feature_activations.numel())\n",
    "    top_k_values, top_k_indices = torch.topk(feature_activations, k)\n",
    "    top_k_words = [words[i] for i in top_k_indices.tolist()]\n",
    "    top_k_activations = top_k_values.tolist()\n",
    "    return list(zip(top_k_words, top_k_activations))\n",
    "\n",
    "def semantic_coherence_score(activated_tokens, activation_scores, word_embeddings):\n",
    "    tokens_and_scores = [(t.lower(), score) for t, score in zip(activated_tokens, activation_scores) if t.lower().isalpha()]\n",
    "    token_embeddings = []\n",
    "    weights = []\n",
    "    for token, score in tokens_and_scores:\n",
    "        if token in word_embeddings:\n",
    "            token_embeddings.append(word_embeddings[token])\n",
    "            weights.append(score)\n",
    "    similarities = []\n",
    "    total_weight = 0\n",
    "    for i in range(len(token_embeddings)):\n",
    "        for j in range(i+1, len(token_embeddings)):\n",
    "            sim = 1 - cosine(token_embeddings[i], token_embeddings[j])\n",
    "            weight = weights[i] * weights[j]\n",
    "            similarities.append(sim * weight)\n",
    "            total_weight += weight\n",
    "    return np.sum(similarities) / total_weight if total_weight > 0 else 0\n",
    "\n",
    "def get_feature_summaries(model, sae, word_embeddings):\n",
    "    W_dec = sae.W_dec.detach().cpu()\n",
    "    W_U_stats_df_dec, dec_projection_onto_W_U = get_W_U_W_dec_stats_df(W_dec, model, cosine_sim=False)\n",
    "    number_of_features = dec_projection_onto_W_U.shape[0]\n",
    "    vocab = model.tokenizer.get_vocab()\n",
    "    words = sorted(vocab.keys(), key=lambda x: vocab[x])\n",
    "    feature_summaries = {}\n",
    "    for i in tqdm(range(number_of_features), desc=\"Processing features\"):\n",
    "        feature_activations = dec_projection_onto_W_U[i]\n",
    "        top_activated_words = get_top_k_words(feature_activations, words)\n",
    "        activated_tokens, activation_scores = zip(*top_activated_words)\n",
    "        coherence_score = semantic_coherence_score(activated_tokens, activation_scores, word_embeddings)\n",
    "        feature_summary = {\n",
    "            \"feature_idx\": i,\n",
    "            \"top_activated_words\": top_activated_words,\n",
    "            \"activation_scores\": activation_scores,\n",
    "            \"coherence_score\": coherence_score\n",
    "        }\n",
    "        feature_summaries[i] = feature_summary\n",
    "    return feature_summaries\n",
    "\n",
    "def calculate_coherence_stats(feature_summaries, feature_ids=None):\n",
    "    if feature_ids is None:\n",
    "        coherence_scores = [summary['coherence_score'] for summary in feature_summaries.values()]\n",
    "    else:\n",
    "        coherence_scores = [feature_summaries[i]['coherence_score'] for i in feature_ids if i in feature_summaries]\n",
    "    coherence_scores = np.array(coherence_scores)\n",
    "    non_zero_scores = coherence_scores[coherence_scores > 0]\n",
    "    stats = {\n",
    "        \"mean_all\": np.mean(coherence_scores),\n",
    "        \"median_all\": np.median(coherence_scores),\n",
    "        \"mean_non_zero\": np.mean(non_zero_scores) if len(non_zero_scores) > 0 else 0,\n",
    "        \"median_non_zero\": np.median(non_zero_scores) if len(non_zero_scores) > 0 else 0,\n",
    "        \"fraction_non_zero\": len(non_zero_scores) / len(coherence_scores),\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing features: 100%|██████████| 24576/24576 [00:22<00:00, 1086.00it/s]\n",
      "Processing features: 100%|██████████| 24576/24576 [00:23<00:00, 1066.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline stats: {'mean_all': 0.07189289731477737, 'median_all': 0.0, 'mean_non_zero': 0.20573497218683492, 'median_non_zero': 0.13993809668288004, 'fraction_non_zero': 0.3882242838541667}\n",
      "Baseline medical stats: {'mean_all': 0.0643932246921882, 'median_all': 0.0, 'mean_non_zero': 0.19984950616088576, 'median_non_zero': 0.13421969825825245, 'fraction_non_zero': 0.3762057877813505}\n",
      "Finetuned stats: {'mean_all': 0.07278746781679458, 'median_all': 0.024560158701740102, 'mean_non_zero': 0.14699870497157008, 'median_non_zero': 0.10255208669743718, 'fraction_non_zero': 0.570556640625}\n",
      "Finetuned medical stats: {'mean_all': 0.0671001134174056, 'median_all': 0.0, 'mean_non_zero': 0.1583993165109143, 'median_non_zero': 0.09375375080888716, 'fraction_non_zero': 0.47123893805309736}\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames for baseline and finetuned data\n",
    "baseline_df = dict_to_df(data['baseline'])\n",
    "finetuned_df = dict_to_df(data['finetuned'])\n",
    "\n",
    "baseline_feature_ids = baseline_df.feature.unique()\n",
    "finetuned_feature_ids = finetuned_df.feature.unique()\n",
    "\n",
    "# Get feature summaries\n",
    "baseline_feature_summaries = get_feature_summaries(model, sae, word_embeddings)\n",
    "finetuned_feature_summaries = get_feature_summaries(custom_model, custom_sae, word_embeddings)\n",
    "\n",
    "# Calculate stats for both models\n",
    "baseline_stats = calculate_coherence_stats(baseline_feature_summaries)\n",
    "baseline_medical_stats = calculate_coherence_stats(baseline_feature_summaries, baseline_feature_ids)\n",
    "finetuned_stats = calculate_coherence_stats(finetuned_feature_summaries)\n",
    "finetuned_medical_stats = calculate_coherence_stats(finetuned_feature_summaries, finetuned_feature_ids)\n",
    "\n",
    "print(\"Baseline stats:\", baseline_stats)\n",
    "print(\"Baseline medical stats:\", baseline_medical_stats)\n",
    "print(\"Finetuned stats:\", finetuned_stats)\n",
    "print(\"Finetuned medical stats:\", finetuned_medical_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Steering Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_steering_hook(steering_on, steering_vector, coeff):\n",
    "    def steering_hook(resid_post, hook):\n",
    "        if steering_on and resid_post.shape[1] > 1:\n",
    "            steering_vector_device = resid_post.device\n",
    "            resid_post[:, :-1, :] += coeff * steering_vector.to(steering_vector_device)\n",
    "    return steering_hook\n",
    "\n",
    "def hooked_generate(prompt_batch, model, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,\n",
    "            input=tokenized,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            verbose=False,\n",
    "            **kwargs)\n",
    "    return result\n",
    "\n",
    "def run_generate(example_prompt, model, hook_point, steering_on, steering_vector):\n",
    "    model.reset_hooks()\n",
    "    steering_hook = create_steering_hook(steering_on, steering_vector, coeff)\n",
    "    editing_hooks = [(hook_point, steering_hook)]\n",
    "    res = hooked_generate([example_prompt], model, editing_hooks, seed=None, **sampling_kwargs)\n",
    "    return model.to_string(res[:, 1:])[0]\n",
    "\n",
    "def keyword_score(text, keywords):\n",
    "    return sum(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "# Set up generation parameters\n",
    "example_prompt = \"What topic is on your mind?\"\n",
    "coeff = 50 \n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "\n",
    "global steering_on\n",
    "global steering_vector\n",
    "\n",
    "def run_experiment(run_title, model, sae, feature_ids, bert_model, tokenizer, feature_summaries):\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(output_dir, f\"{timestamp}_coeff_{coeff}_{run_title}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Create cfg.json\n",
    "    cfg = {\n",
    "        \"name\": run_title,\n",
    "        \"start_time\": timestamp,\n",
    "        \"coeff\": coeff,\n",
    "        \"example_prompt\": example_prompt,\n",
    "        \"sampling_kwargs\": sampling_kwargs\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"cfg.json\"), \"w\") as f:\n",
    "        json.dump(cfg, f, indent=4)\n",
    "\n",
    "    def wrap_text(text, width=80):\n",
    "        return \"\\n\".join(textwrap.wrap(text, width=width))\n",
    "\n",
    "    steered_wins = 0\n",
    "    baseline_wins = 0\n",
    "    steered_sims = []\n",
    "    baseline_sims = []\n",
    "\n",
    "    for feature_idx in tqdm(feature_ids, desc=\"Processing features\"):\n",
    "        hook_point = sae.cfg.hook_name\n",
    "        steering_vector = sae.W_dec[feature_idx]\n",
    "        \n",
    "        steered_text = run_generate(example_prompt, model, hook_point, True, steering_vector)\n",
    "        baseline_text = run_generate(example_prompt, model, hook_point, False, steering_vector)\n",
    "        \n",
    "        output = f\"\\n{'='*100}\\n\"\n",
    "        output += f\"FEATURE {feature_idx:>3}\\n\"\n",
    "        output += f\"{'='*100}\\n\\n\"\n",
    "\n",
    "        output += \"1. Steered Output:\\n\"\n",
    "        output += f\"   {wrap_text(steered_text)}\\n\\n\"\n",
    "\n",
    "        output += \"2. Baseline Output:\\n\"\n",
    "        output += f\"   {wrap_text(baseline_text)}\\n\\n\"\n",
    "\n",
    "        medical_keywords = [word for word, _ in finetuned_feature_summaries[feature_idx]['top_activated_words']]\n",
    "        \n",
    "        output += \"3. Top Activating Tokens:\\n\"\n",
    "        output += \"   \" + \", \".join(medical_keywords[:10]) + \"\\n\"\n",
    "        output += f\"   (Total: {len(medical_keywords)} tokens)\\n\\n\"\n",
    "\n",
    "        reference_text = \" \".join(medical_keywords)\n",
    "\n",
    "        baseline_score = keyword_score(baseline_text, medical_keywords)\n",
    "        steered_score = keyword_score(steered_text, medical_keywords)\n",
    "\n",
    "        baseline_embedding = get_embedding(baseline_text, bert_model, tokenizer)\n",
    "        steered_embedding = get_embedding(steered_text, bert_model, tokenizer)\n",
    "        reference_embedding = get_embedding(reference_text, bert_model, tokenizer)\n",
    "\n",
    "        baseline_similarity = cosine_similarity(baseline_embedding.unsqueeze(0), reference_embedding.unsqueeze(0)).item()\n",
    "        steered_similarity = cosine_similarity(steered_embedding.unsqueeze(0), reference_embedding.unsqueeze(0)).item()\n",
    "\n",
    "        baseline_sims.append(baseline_similarity)\n",
    "        steered_sims.append(steered_similarity)\n",
    "\n",
    "        output += \"4. Scores:\\n\"\n",
    "        output += f\"   Keyword Scores      - Baseline: {baseline_score:>4}  |  Steered: {steered_score:>4}\\n\"\n",
    "        output += f\"   Semantic Similarity - Baseline: {baseline_similarity:.4f}  |  Steered: {steered_similarity:.4f}\\n\"\n",
    "        output += '   ' + '-'*60 + '\\n'\n",
    "        \n",
    "        if steered_similarity > baseline_similarity:\n",
    "            output += \"   RESULT: The steered output is more aligned with the feature.\\n\"\n",
    "            steered_wins += 1\n",
    "        elif steered_similarity == baseline_similarity:\n",
    "            output += \"   RESULT: No significant difference in alignment between outputs.\\n\"\n",
    "        else:\n",
    "            output += \"   RESULT: The baseline output is more aligned with the feature.\\n\"\n",
    "            baseline_wins += 1\n",
    "        \n",
    "        with open(os.path.join(run_dir, f\"feature_{feature_idx}.txt\"), \"w\") as f:\n",
    "            f.write(output)\n",
    "\n",
    "    print(f\"Steered wins: {steered_wins}\")\n",
    "    print(f\"Baseline wins: {baseline_wins}\")\n",
    "\n",
    "    # Generate final report\n",
    "    final_report = {\n",
    "        \"total_features_analyzed\": len(feature_ids),\n",
    "        \"steered_wins\": steered_wins,\n",
    "        \"baseline_wins\": baseline_wins,\n",
    "        \"ties\": len(feature_ids) - steered_wins - baseline_wins,\n",
    "        \"average_steered_similarity\": sum(steered_sims) / len(steered_sims),\n",
    "        \"average_baseline_similarity\": sum(baseline_sims) / len(baseline_sims)\n",
    "    }\n",
    "\n",
    "    # Write final report to JSON file\n",
    "    with open(os.path.join(run_dir, \"final_report.json\"), \"w\") as f:\n",
    "        json.dump(final_report, f, indent=4)\n",
    "\n",
    "    # Print final results to stdout\n",
    "    print(json.dumps(final_report, indent=4))\n",
    "\n",
    "    # Update cfg.json with end time and results\n",
    "    cfg.update({\n",
    "        \"end_time\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        \"steered_wins\": steered_wins,\n",
    "        \"baseline_wins\": baseline_wins,\n",
    "        \"ties\": len(feature_ids) - steered_wins - baseline_wins,\n",
    "        \"avg_steered_similarity\": sum(steered_sims) / len(steered_sims),\n",
    "        \"avg_baseline_similarity\": sum(baseline_sims) / len(baseline_sims)\n",
    "    })\n",
    "\n",
    "    with open(os.path.join(run_dir, \"cfg.json\"), \"w\") as f:\n",
    "        json.dump(cfg, f, indent=4)\n",
    "\n",
    "    # Return the data needed for regression analysis\n",
    "    return {\n",
    "        'feature_ids': feature_ids,\n",
    "        'steered_sims': steered_sims,\n",
    "        'baseline_sims': baseline_sims,\n",
    "        'feature_summaries': feature_summaries,\n",
    "        'final_report': final_report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features: 100%|██████████| 311/311 [21:18<00:00,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered wins: 158\n",
      "Baseline wins: 153\n",
      "{\n",
      "    \"total_features_analyzed\": 311,\n",
      "    \"steered_wins\": 158,\n",
      "    \"baseline_wins\": 153,\n",
      "    \"ties\": 0,\n",
      "    \"average_steered_similarity\": 0.4558351784465397,\n",
      "    \"average_baseline_similarity\": 0.45490066328616\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run experiments\n",
    "baseline_results = run_experiment(\"baseline\", model, sae, baseline_feature_ids, bert_model, tokenizer, baseline_feature_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = run_experiment(\"finetuned\", custom_model, custom_sae, finetuned_feature_ids, bert_model, tokenizer, finetuned_feature_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "baseline_coherence, baseline_similarity = prepare_regression_data(\n",
    "    baseline_results['feature_summaries'], \n",
    "    baseline_results['feature_ids'], \n",
    "    baseline_results['baseline_sims']\n",
    ")\n",
    "\n",
    "finetuned_coherence, finetuned_similarity = prepare_regression_data(\n",
    "    finetuned_results['feature_summaries'], \n",
    "    finetuned_results['feature_ids'], \n",
    "    finetuned_results['steered_sims']\n",
    ")\n",
    "\n",
    "# Perform regression and plotting for baseline model\n",
    "baseline_slope, baseline_intercept, baseline_r_squared, baseline_p_value = perform_regression(\n",
    "    baseline_coherence, baseline_similarity)\n",
    "\n",
    "plot_regression(baseline_coherence, baseline_similarity, \n",
    "                baseline_slope, baseline_intercept, baseline_r_squared, baseline_p_value,\n",
    "                'Baseline Model: Feature Coherence vs Cosine Similarity')\n",
    "\n",
    "print(f\"Baseline Model Results:\")\n",
    "print(f\"Slope: {baseline_slope:.4f}\")\n",
    "print(f\"Intercept: {baseline_intercept:.4f}\")\n",
    "print(f\"R-squared: {baseline_r_squared:.4f}\")\n",
    "print(f\"p-value: {baseline_p_value:.4e}\")\n",
    "\n",
    "# Perform regression and plotting for fine-tuned model\n",
    "finetuned_slope, finetuned_intercept, finetuned_r_squared, finetuned_p_value = perform_regression(\n",
    "    finetuned_coherence, finetuned_similarity)\n",
    "\n",
    "plot_regression(finetuned_coherence, finetuned_similarity, \n",
    "                finetuned_slope, finetuned_intercept, finetuned_r_squared, finetuned_p_value,\n",
    "                'Fine-tuned Model: Feature Coherence vs Cosine Similarity')\n",
    "\n",
    "print(f\"\\nFine-tuned Model Results:\")\n",
    "print(f\"Slope: {finetuned_slope:.4f}\")\n",
    "print(f\"Intercept: {finetuned_intercept:.4f}\")\n",
    "print(f\"R-squared: {finetuned_r_squared:.4f}\")\n",
    "print(f\"p-value: {finetuned_p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has analyzed the relationship between feature coherence scores and vector steering outcomes for both baseline and fine-tuned GPT-2 models. The regression analysis provides insights into how feature coherence relates to the effectiveness of vector steering in each model.\n",
    "\n",
    "Key points to consider in the interpretation:\n",
    "\n",
    "1. Slope: Indicates the change in cosine similarity for a one-unit change in feature coherence.\n",
    "2. R-squared: Measures the proportion of variance in cosine similarity explained by feature coherence.\n",
    "3. p-value: Indicates the statistical significance of the relationship.\n",
    "\n",
    "Compare the results between the baseline and fine-tuned models to determine if there's a stronger relationship between feature coherence and steering vector outcomes in one model versus the other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
