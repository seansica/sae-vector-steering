{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jaxtyping in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (0.2.25)\n",
      "Requirement already satisfied: transformer_lens in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (2.2.1)\n",
      "Collecting plotly-utils\n",
      "  Using cached plotly_utils-0.0.3-py3-none-any.whl.metadata (796 bytes)\n",
      "Requirement already satisfied: einops in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: torch in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jaxtyping) (1.26.4)\n",
      "Requirement already satisfied: typeguard<3,>=2.13.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jaxtyping) (2.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jaxtyping) (4.12.2)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (0.26.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (2.20.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (2.2.0)\n",
      "Requirement already satisfied: rich>=12.6.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (13.7.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (4.66.4)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (4.42.4)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer_lens) (0.16.2)\n",
      "Requirement already satisfied: statsmodels>=0.10.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from plotly-utils) (0.14.2)\n",
      "Requirement already satisfied: plotly>=4.1.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from plotly-utils) (5.22.0)\n",
      "Requirement already satisfied: filelock in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets>=2.7.1->transformer_lens) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from plotly>=4.1.1->plotly-utils) (8.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from rich>=12.6.0->transformer_lens) (2.17.2)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from statsmodels>=0.10.1->plotly-utils) (1.13.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from statsmodels>=0.10.1->plotly-utils) (0.5.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformers>=4.37.2->transformer_lens) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.41)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (1.39.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (69.0.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer_lens) (4.25.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2023.11.17)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
      "Using cached plotly_utils-0.0.3-py3-none-any.whl (3.1 kB)\n",
      "Installing collected packages: plotly-utils\n",
      "Successfully installed plotly-utils-0.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/callummcdougall/eindex.git\n",
      "  Cloning https://github.com/callummcdougall/eindex.git to /private/var/folders/3c/gk783fjn6g7fhss6pg10m_q40000gn/T/pip-req-build-hji5o24j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/eindex.git /private/var/folders/3c/gk783fjn6g7fhss6pg10m_q40000gn/T/pip-req-build-hji5o24j\n",
      "  Resolved https://github.com/callummcdougall/eindex.git to commit 39da17a59dcce5caee35c9d00d4a22ba33efe722\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from eindex-callum==0.1.1) (2.1.2)\n",
      "Requirement already satisfied: einops in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from eindex-callum==0.1.1) (0.7.0)\n",
      "Requirement already satisfied: filelock in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch->eindex-callum==0.1.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch->eindex-callum==0.1.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch->eindex-callum==0.1.1) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch->eindex-callum==0.1.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch->eindex-callum==0.1.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch->eindex-callum==0.1.1) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jinja2->torch->eindex-callum==0.1.1) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sympy->torch->eindex-callum==0.1.1) (1.3.0)\n",
      "Building wheels for collected packages: eindex-callum\n",
      "  Building wheel for eindex-callum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for eindex-callum: filename=eindex_callum-0.1.1-py3-none-any.whl size=8325 sha256=afb14148cbefd1f1c079277153cb4a02e0dbdae711b6bf52cb802aecb4e360ae\n",
      "  Stored in directory: /private/var/folders/3c/gk783fjn6g7fhss6pg10m_q40000gn/T/pip-ephem-wheel-cache-_o_v1nr9/wheels/7c/77/b4/5cc5d177c0a6c79129a36ef030f78531700100e0f742c7e812\n",
      "Successfully built eindex-callum\n",
      "Installing collected packages: eindex-callum\n",
      "Successfully installed eindex-callum-0.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jaxtyping transformer_lens plotly-utils einops torch\n",
    "%pip install git+https://github.com/callummcdougall/eindex.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Optional, Callable, Union, List, Tuple\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "\n",
    "# from plotly_utils import imshow, line, hist\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_instances models in a single training loop to let us sweep over\n",
    "    # sparsity or importance curves  efficiently. You should treat `n_instances` as\n",
    "    # kinda like a batch dimension, but one which is built into our training setup.\n",
    "    n_instances: int\n",
    "    n_features: int = 5\n",
    "    n_hidden: int = 2\n",
    "    n_correlated_pairs: int = 0\n",
    "    n_anticorrelated_pairs: int = 0\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "    # Our linear map is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Union[float, Tensor]] = None,\n",
    "        importance: Optional[Union[float, Tensor]] = None,\n",
    "        device = device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if feature_probability is None: feature_probability = t.ones(())\n",
    "        if isinstance(feature_probability, float): feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to((cfg.n_instances, cfg.n_features))\n",
    "        if importance is None: importance = t.ones(())\n",
    "        if isinstance(importance, float): importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_instances, cfg.n_features))\n",
    "\n",
    "        self.W = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features))))\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_instances, cfg.n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "        hidden = einops.einsum(\n",
    "           features, self.W,\n",
    "           \"... instances features, instances hidden features -> ... instances hidden\"\n",
    "        )\n",
    "        out = einops.einsum(\n",
    "            hidden, self.W,\n",
    "            \"... instances hidden, instances hidden features -> ... instances features\"\n",
    "        )\n",
    "        return F.relu(out + self.b_final)\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "        '''\n",
    "        Generates a batch of data. We'll return to this function later when we apply correlations.\n",
    "        '''\n",
    "        # Generate the features, before randomly setting some to zero\n",
    "        feat = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W.device)\n",
    "\n",
    "        # Generate a random boolean array, which is 1 wherever we'll keep a feature, and zero where we'll set it to zero\n",
    "        feat_seeds = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W.device)\n",
    "        feat_is_present = feat_seeds <= self.feature_probability\n",
    "\n",
    "        # Create our batch from the features, where we set some to zero\n",
    "        batch = t.where(feat_is_present, feat, 0.0)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        '''\n",
    "        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Remember, `self.importance` will always have shape (n_instances, n_features).\n",
    "        '''\n",
    "        error = self.importance * ((batch - out) ** 2)\n",
    "        loss = einops.reduce(error, 'batch instances features -> instances', 'mean').sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        '''\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        '''\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        for step in progress_bar:\n",
    "\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(loss=loss.item()/self.cfg.n_instances, lr=step_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AutoEncoderConfig:\n",
    "    n_instances: int\n",
    "    n_input_ae: int\n",
    "    n_hidden_ae: int\n",
    "    l1_coeff: float = 0.5\n",
    "    tied_weights: bool = False\n",
    "    weight_normalize_eps: float = 1e-8\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    W_enc: Float[Tensor, \"n_instances n_input_ae n_hidden_ae\"]\n",
    "    W_dec: Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]\n",
    "    b_enc: Float[Tensor, \"n_instances n_hidden_ae\"]\n",
    "    b_dec: Float[Tensor, \"n_instances n_input_ae\"]\n",
    "\n",
    "\n",
    "    def __init__(self, cfg: AutoEncoderConfig):\n",
    "        '''\n",
    "        Initializes the two weights and biases according to the type signature above.\n",
    "\n",
    "        If self.cfg.tied_weights = True, then we only create W_enc, not W_dec.\n",
    "        '''\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.W_enc = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_input_ae, cfg.n_hidden_ae))))\n",
    "        if not(cfg.tied_weights):\n",
    "            self.W_dec = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden_ae, cfg.n_input_ae))))\n",
    "\n",
    "        self.b_enc = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden_ae))\n",
    "        self.b_dec = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_input_ae))\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def normalize_and_return_W_dec(self) -> Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]:\n",
    "        '''\n",
    "        If self.cfg.tied_weights = True, we return the normalized & transposed encoder weights.\n",
    "        If self.cfg.tied_weights = False, we normalize the decoder weights in-place, and return them.\n",
    "\n",
    "        Normalization should be over the `n_input_ae` dimension, i.e. each feature should have a noramlized decoder weight.\n",
    "        '''\n",
    "        if self.cfg.tied_weights:\n",
    "            return self.W_enc.transpose(-1, -2) / (self.W_enc.transpose(-1, -2).norm(dim=1, keepdim=True) + self.cfg.weight_normalize_eps)\n",
    "        else:\n",
    "            self.W_dec.data = self.W_dec.data / (self.W_dec.data.norm(dim=2, keepdim=True) + self.cfg.weight_normalize_eps)\n",
    "            return self.W_dec\n",
    "\n",
    "\n",
    "    def forward(self, h: Float[Tensor, \"batch_size n_instances n_input_ae\"]):\n",
    "        '''\n",
    "        Runs a forward pass on the autoencoder, and returns several outputs.\n",
    "\n",
    "        Inputs:\n",
    "            h: Float[Tensor, \"batch_size n_instances n_input_ae\"]\n",
    "                hidden activations generated from a Model instance\n",
    "\n",
    "        Returns:\n",
    "            l1_loss: Float[Tensor, \"batch_size n_instances\"]\n",
    "                L1 loss for each batch elem & each instance (sum over the `n_hidden_ae` dimension)\n",
    "            l2_loss: Float[Tensor, \"batch_size n_instances\"]\n",
    "                L2 loss for each batch elem & each instance (take mean over the `n_input_ae` dimension)\n",
    "            loss: Float[Tensor, \"\"]\n",
    "                Sum of L1 and L2 loss (with the former scaled by `self.cfg.l1_coeff). We sum over the `n_instances`\n",
    "                dimension but take mean over the batch dimension\n",
    "            acts: Float[Tensor, \"batch_size n_instances n_hidden_ae\"]\n",
    "                Activations of the autoencoder's hidden states (post-ReLU)\n",
    "            h_reconstructed: Float[Tensor, \"batch_size n_instances n_input_ae\"]\n",
    "                Reconstructed hidden states, i.e. the autoencoder's final output\n",
    "        '''\n",
    "        # Compute activations\n",
    "        h_cent = h - self.b_dec\n",
    "        acts = einops.einsum(\n",
    "            h_cent, self.W_enc,\n",
    "            \"batch_size n_instances n_input_ae, n_instances n_input_ae n_hidden_ae -> batch_size n_instances n_hidden_ae\"\n",
    "        )\n",
    "        acts = F.relu(acts + self.b_enc)\n",
    "\n",
    "        # Compute reconstructed input\n",
    "        h_reconstructed = einops.einsum(\n",
    "            acts, self.normalize_and_return_W_dec(),\n",
    "            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n",
    "        ) + self.b_dec\n",
    "\n",
    "        # Compute loss, return values\n",
    "        l2_loss = (h_reconstructed - h).pow(2).mean(-1) # shape [batch_size n_instances]\n",
    "        l1_loss = acts.abs().sum(-1) # shape [batch_size n_instances]\n",
    "        loss = (self.cfg.l1_coeff * l1_loss + l2_loss).mean(0).sum() # scalar\n",
    "\n",
    "        return l1_loss, l2_loss, loss, acts, h_reconstructed\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        model: Model,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "        neuron_resample_window: Optional[int] = None,\n",
    "        dead_neuron_window: Optional[int] = None,\n",
    "        neuron_resample_scale: float = 0.2,\n",
    "    ):\n",
    "        '''\n",
    "        Optimizes the autoencoder using the given hyperparameters.\n",
    "\n",
    "        The autoencoder is trained on the hidden state activations produced by 'model', and it\n",
    "        learns to reconstruct the features which this model represents in superposition.\n",
    "        '''\n",
    "        if neuron_resample_window is not None:\n",
    "            assert (dead_neuron_window is not None) and (dead_neuron_window < neuron_resample_window)\n",
    "\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "        frac_active_list = []\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        # Create lists to store data we'll eventually be plotting\n",
    "        data_log = {\"W_enc\": [], \"W_dec\": [], \"colors\": [], \"titles\": [], \"frac_active\": []}\n",
    "        colors = None\n",
    "        title = \"no resampling yet\"\n",
    "\n",
    "        for step in progress_bar:\n",
    "\n",
    "            # Update learning rate based on `lr_scale` function\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = step_lr\n",
    "\n",
    "            # Get a batch of hidden activations from the model (for the training step & the neuron resampling)\n",
    "            with t.inference_mode():\n",
    "                features = model.generate_batch(batch_size)\n",
    "                h = einops.einsum(features, model.W, \"batch instances feats, instances hidden feats -> batch instances hidden\")\n",
    "\n",
    "            # Resample dead neurons\n",
    "            if (neuron_resample_window is not None) and ((step + 1) % neuron_resample_window == 0):\n",
    "                # Get the fraction of neurons active in the previous window\n",
    "                frac_active_in_window = t.stack(frac_active_list[-neuron_resample_window:], dim=0)\n",
    "                # Apply resampling\n",
    "                colors, title = self.resample_neurons(h, frac_active_in_window, neuron_resample_scale)\n",
    "\n",
    "            # Optimize\n",
    "            l1_loss, l2_loss, loss, acts, _ = self.forward(h)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the mean sparsities over batch dim for each (instance, feature)\n",
    "            frac_active = (acts.abs() > 1e-8).float().mean(0)\n",
    "            frac_active_list.append(frac_active)\n",
    "\n",
    "            # Display progress bar, and append new values for plotting\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(l1_loss=self.cfg.l1_coeff * l1_loss.mean(0).sum().item(), l2_loss=l2_loss.mean(0).sum().item(), lr=step_lr)\n",
    "                data_log[\"W_enc\"].append(self.W_enc.detach().cpu().clone())\n",
    "                data_log[\"W_dec\"].append(self.normalize_and_return_W_dec().detach().cpu().clone())\n",
    "                data_log[\"colors\"].append(colors)\n",
    "                data_log[\"titles\"].append(f\"Step {step}/{steps}: {title}\")\n",
    "                data_log[\"frac_active\"].append(frac_active.detach().cpu().clone())\n",
    "\n",
    "        return data_log\n",
    "\n",
    "\n",
    "    @t.no_grad()\n",
    "    def resample_neurons(\n",
    "        self,\n",
    "        h: Float[Tensor, \"batch_size n_instances n_input_ae\"],\n",
    "        frac_active_in_window: Float[Tensor, \"window n_instances n_hidden_ae\"],\n",
    "        neuron_resample_scale: float,\n",
    "    ) -> Tuple[List[List[str]], str]:\n",
    "        '''\n",
    "        Resamples neurons that have been dead for 'dead_neuron_window' steps, according to `frac_active`.\n",
    "        '''\n",
    "        # Get a tensor of dead neurons\n",
    "        dead_features_mask = frac_active_in_window.sum(0) < 1e-8 # shape [instances hidden_ae]\n",
    "        n_dead = dead_features_mask.int().sum().item()\n",
    "\n",
    "        # Get our random replacement values\n",
    "        replacement_values = t.randn((n_dead, self.cfg.n_input_ae), device=self.W_enc.device) # shape [n_dead n_input_ae]\n",
    "        replacement_values_normalized = replacement_values / (replacement_values.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # Change the corresponding values in W_enc, W_dec, and b_enc (note we transpose W_enc to return a view with correct shape)\n",
    "        self.W_enc.data.transpose(-1, -2)[dead_features_mask] = replacement_values_normalized\n",
    "        self.W_dec.data[dead_features_mask] = replacement_values_normalized\n",
    "        self.b_enc.data[dead_features_mask] = 0.0\n",
    "\n",
    "        # Return data for visualising the resampling process\n",
    "        colors = [[\"red\" if dead else \"black\" for dead in dead_neuron_mask_inst] for dead_neuron_mask_inst in dead_features_mask]\n",
    "        title = f\"resampling {n_dead}/{dead_features_mask.numel()} neurons (shown in red)\"\n",
    "        return colors, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "from transformer_lens.utils import (\n",
    "    load_dataset,\n",
    "    tokenize_and_concatenate,\n",
    "    download_file_from_hf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7477bc6c9f43459621271eb650f0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "25_cfg.json:   0%|          | 0.00/283 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fc47b53071455096617343f622ffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "25.pt:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a27b9c232b41f98785f902a3533602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "47_cfg.json:   0%|          | 0.00/309 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1206fbbc87ff4dd28b45b47e52283483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "47.pt:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VERSION_DICT = {\"run1\": 25, \"run2\": 47}\n",
    "\n",
    "\n",
    "def load_autoencoder_from_huggingface(versions: List[str] = [\"run1\", \"run2\"]):\n",
    "    state_dict = {}\n",
    "\n",
    "    for version in versions:\n",
    "        version_id = VERSION_DICT[version]\n",
    "        # Load the data from huggingface (both metadata and state dict)\n",
    "        sae_data: dict = download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version_id}_cfg.json\")\n",
    "        new_state_dict: dict = download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version_id}.pt\", force_is_torch=True)\n",
    "        # Add new state dict to the existing one\n",
    "        for k, v in new_state_dict.items():\n",
    "            state_dict[k] = t.stack([state_dict[k], v]) if k in state_dict else v\n",
    "\n",
    "    # Get data about the model dimensions, and use that to initialize our model (with 2 instances)\n",
    "    d_mlp = sae_data[\"d_mlp\"]\n",
    "    dict_mult = sae_data[\"dict_mult\"]\n",
    "    n_hidden_ae = d_mlp * dict_mult\n",
    "\n",
    "    cfg = AutoEncoderConfig(\n",
    "        n_instances = 2,\n",
    "        n_input_ae = d_mlp,\n",
    "        n_hidden_ae = n_hidden_ae,\n",
    "    )\n",
    "\n",
    "    # Initialize our model, and load in state dict\n",
    "    autoencoder = AutoEncoder(cfg)\n",
    "    autoencoder.load_state_dict(state_dict)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "autoencoder = load_autoencoder_from_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "print(\"Tokens shape: \", all_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def highest_activating_tokens(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    "    feature_idx: int,\n",
    "    autoencoder_B: bool = False,\n",
    "    k: int = 10,\n",
    ") -> Tuple[Int[Tensor, \"k 2\"], Float[Tensor, \"k\"]]:\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    instance_idx = 1 if autoencoder_B else 0\n",
    "\n",
    "    # Get the post activations from the clean run\n",
    "    cache = model.run_with_cache(tokens, names_filter=[\"blocks.0.mlp.hook_post\"])[1]\n",
    "    post = cache[\"blocks.0.mlp.hook_post\"]\n",
    "    post_reshaped = einops.rearrange(post, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    h_cent = post_reshaped - autoencoder.b_dec[instance_idx]\n",
    "    acts = einops.einsum(\n",
    "        h_cent, autoencoder.W_enc[instance_idx, :, feature_idx],\n",
    "        \"batch_size n_input_ae, n_input_ae -> batch_size\"\n",
    "    )\n",
    "\n",
    "    # Get the top k largest activations\n",
    "    top_acts_values, top_acts_indices = acts.topk(k)\n",
    "\n",
    "    # Convert the indices into (batch, seq) indices\n",
    "    top_acts_batch = top_acts_indices // seq_len\n",
    "    top_acts_seq = top_acts_indices % seq_len\n",
    "\n",
    "    return t.stack([top_acts_batch, top_acts_seq], dim=-1), top_acts_values\n",
    "\n",
    "\n",
    "def display_top_sequences(top_acts_indices, top_acts_values, tokens):\n",
    "    table = Table(\"Sequence\", \"Activation\", title=\"Tokens which most activate this feature\")\n",
    "    for (batch_idx, seq_idx), value in zip(top_acts_indices, top_acts_values):\n",
    "        # Get the sequence as a string (with some padding on either side of our sequence)\n",
    "        seq = \"\"\n",
    "        for i in range(max(seq_idx-5, 0), min(seq_idx+5, all_tokens.shape[1])):\n",
    "            new_str_token = model.to_single_str_token(tokens[batch_idx, i].item()).replace(\"\\n\", \"\\\\n\")\n",
    "            # Highlight the token with the high activation\n",
    "            if i == seq_idx: new_str_token = f\"[b u dark_orange]{new_str_token}[/]\"\n",
    "            seq += new_str_token\n",
    "        # Print the sequence, and the activation value\n",
    "        table.add_row(seq, f'{value:.2f}')\n",
    "    rprint(table)\n",
    "\n",
    "tokens = all_tokens[:200]\n",
    "\n",
    "# for feature_idx in range(5,10):\n",
    "#   top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, autoencoder, feature_idx=feature_idx, autoencoder_B=False)\n",
    "#   display_top_sequences(top_acts_indices, top_acts_values, tokens)\n",
    "\n",
    "top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, autoencoder, feature_idx=7, autoencoder_B=False)\n",
    "display_top_sequences(top_acts_indices, top_acts_values, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dec_vector = autoencoder.W_dec[0, 7]\n",
    "\n",
    "W_dec_logits = W_dec_vector @ model.W_out[0] @ model.W_U\n",
    "\n",
    "top_tokens = W_dec_logits.topk(10)\n",
    "bottom_tokens = W_dec_logits.topk(10, largest=False)\n",
    "\n",
    "s = \"Top tokens:\\n\"\n",
    "for token, value in zip(top_tokens.indices, top_tokens.values):\n",
    "    s += f\"({value:.2f}) {model.to_single_str_token(token.item())}\\n\"\n",
    "s += \"\\nBottom tokens:\\n\"\n",
    "for token, value in zip(bottom_tokens.indices, bottom_tokens.values):\n",
    "    s += f\"({value:.2f}) {model.to_single_str_token(token.item())}\\n\"\n",
    "rprint(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature we want to:\n",
    "\n",
    "1. Get the indices and values for the topK activations\n",
    "    \n",
    "    * Get post activations from a clean run\n",
    "    \n",
    "    * Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "\n",
    "2. For each activation tuple (index and value), get the token as a deserialized/human-readable string with some padding/context and the activation logit (indicating the activation power)\n",
    "\n",
    "We need to code up a solution that lets us do this for many features, iteratively.\n",
    "\n",
    "Importantly, we want to store the results in a data structure that can be (1) easily parsed, and (2) easily serialized to JSON so that it can be fed to an LLM for deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from typing import List, Dict, NamedTuple\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class SeqActivation(NamedTuple):\n",
    "    context: str\n",
    "    seq: str\n",
    "    activation: float\n",
    "\n",
    "class FeatureActivations(NamedTuple):\n",
    "    feature_idx: int\n",
    "    top_10_activations: List[SeqActivation]\n",
    "    bottom_10_activations: List[SeqActivation]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_feature_activations(\n",
    "    tokens: torch.Tensor,\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    "    feature_idx: int,\n",
    "    k: int = 10\n",
    ") -> FeatureActivations:\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    \n",
    "    # Get the post activations from the clean run\n",
    "    cache = model.run_with_cache(tokens, names_filter=[\"blocks.0.mlp.hook_post\"])[1]\n",
    "    post = cache[\"blocks.0.mlp.hook_post\"]\n",
    "    post_reshaped = einops.rearrange(post, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "\n",
    "    # Compute activations for this feature\n",
    "    h_cent = post_reshaped - autoencoder.b_dec[0]\n",
    "    acts = einops.einsum(\n",
    "        h_cent, autoencoder.W_enc[0, :, feature_idx],\n",
    "        \"batch_size n_input_ae, n_input_ae -> batch_size\"\n",
    "    )\n",
    "\n",
    "    # Get the top and bottom k activations\n",
    "    top_acts_values, top_acts_indices = acts.topk(k)\n",
    "    bottom_acts_values, bottom_acts_indices = acts.topk(k, largest=False)\n",
    "\n",
    "    def create_seq_activations(values, indices):\n",
    "        seq_activations = []\n",
    "        for idx, value in zip(indices, values):\n",
    "            batch_idx = idx // seq_len\n",
    "            seq_idx = idx % seq_len\n",
    "            \n",
    "            context = \"\"\n",
    "            for i in range(max(seq_idx-5, 0), min(seq_idx+5, tokens.shape[1])):\n",
    "                token = model.to_single_str_token(tokens[batch_idx, i].item()).replace(\"\\n\", \"\\\\n\")\n",
    "                if i == seq_idx:\n",
    "                    seq = token\n",
    "                context += token\n",
    "\n",
    "            seq_activations.append(SeqActivation(context=context, seq=seq, activation=value.item()))\n",
    "        \n",
    "        return seq_activations\n",
    "\n",
    "    top_10_activations = create_seq_activations(top_acts_values, top_acts_indices)\n",
    "    bottom_10_activations = create_seq_activations(bottom_acts_values, bottom_acts_indices)\n",
    "\n",
    "    return FeatureActivations(\n",
    "        feature_idx=feature_idx,\n",
    "        top_10_activations=top_10_activations,\n",
    "        bottom_10_activations=bottom_10_activations\n",
    "    )\n",
    "\n",
    "def collect_feature_activations(\n",
    "    tokens: torch.Tensor,\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    "    num_features: int\n",
    ") -> List[FeatureActivations]:\n",
    "    feature_activations = []\n",
    "    for feature_idx in range(num_features):\n",
    "        activations = get_feature_activations(tokens, model, autoencoder, feature_idx)\n",
    "        feature_activations.append(activations)\n",
    "        console.print(f\"Processed feature {feature_idx + 1}/{num_features}\")\n",
    "    return feature_activations\n",
    "\n",
    "# Usage\n",
    "tokens = all_tokens[:200]  # Adjust as needed\n",
    "num_features_to_analyze = 10  # Adjust as needed\n",
    "\n",
    "feature_activations = collect_feature_activations(tokens, model, autoencoder, num_features_to_analyze)\n",
    "\n",
    "# Display results\n",
    "for feature in feature_activations:\n",
    "    console.print(f\"\\nFeature {feature.feature_idx}:\")\n",
    "    \n",
    "    table = Table(title=f\"Top 10 Activations for Feature {feature.feature_idx}\")\n",
    "    table.add_column(\"Context\", style=\"cyan\")\n",
    "    table.add_column(\"Sequence\", style=\"magenta\")\n",
    "    table.add_column(\"Activation\", justify=\"right\", style=\"green\")\n",
    "    \n",
    "    for activation in feature.top_10_activations:\n",
    "        table.add_row(activation.context, activation.seq, f\"{activation.activation:.4f}\")\n",
    "    \n",
    "    console.print(table)\n",
    "\n",
    "    table = Table(title=f\"Bottom 10 Activations for Feature {feature.feature_idx}\")\n",
    "    table.add_column(\"Context\", style=\"cyan\")\n",
    "    table.add_column(\"Sequence\", style=\"magenta\")\n",
    "    table.add_column(\"Activation\", justify=\"right\", style=\"red\")\n",
    "    \n",
    "    for activation in feature.bottom_10_activations:\n",
    "        table.add_row(activation.context, activation.seq, f\"{activation.activation:.4f}\")\n",
    "    \n",
    "    console.print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
