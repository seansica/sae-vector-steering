{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sae-lens\n",
      "  Downloading sae_lens-3.12.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting automated-interpretability<0.0.4,>=0.0.3 (from sae-lens)\n",
      "  Downloading automated_interpretability-0.0.3-py3-none-any.whl.metadata (817 bytes)\n",
      "Collecting babe<0.0.8,>=0.0.7 (from sae-lens)\n",
      "  Downloading babe-0.0.7-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting datasets<3.0.0,>=2.17.1 (from sae-lens)\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sae-lens) (3.9.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sae-lens) (0.1.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sae-lens) (3.8.1)\n",
      "Collecting plotly<6.0.0,>=5.19.0 (from sae-lens)\n",
      "  Downloading plotly-5.22.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting plotly-express<0.5.0,>=0.4.1 (from sae-lens)\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pytest-profiling<2.0.0,>=1.7.0 (from sae-lens)\n",
      "  Downloading pytest_profiling-1.7.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from sae-lens)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sae-lens) (6.0.1)\n",
      "Collecting pyzmq==26.0.0 (from sae-lens)\n",
      "  Downloading pyzmq-26.0.0-cp311-cp311-macosx_10_15_universal2.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: safetensors<0.5.0,>=0.4.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sae-lens) (0.4.2)\n",
      "Collecting transformer-lens<3.0.0,>=2.0.0 (from sae-lens)\n",
      "  Downloading transformer_lens-2.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers<5.0.0,>=4.38.1 (from sae-lens)\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.13.0,>=0.12.3 (from sae-lens)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.10.0 (from sae-lens)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: zstandard<0.23.0,>=0.22.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sae-lens) (0.22.0)\n",
      "Collecting blobfile<3.0.0,>=2.1.1 (from automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading blobfile-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting boostedblob<0.16.0,>=0.15.3 (from automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading boostedblob-0.15.4-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens) (0.27.0)\n",
      "Collecting numpy<2.0.0,>=1.26.4 (from automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting orjson<4.0.0,>=3.10.1 (from automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading orjson-3.10.6-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest<9.0.0,>=8.1.2 (from automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading pytest-8.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.4.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens) (1.4.2)\n",
      "Collecting tiktoken<0.7.0,>=0.6.0 (from automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pandas in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from babe<0.0.8,>=0.0.7->sae-lens) (2.2.0)\n",
      "Collecting py2store (from babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading py2store-0.1.20.tar.gz (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting graze (from babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading graze-0.1.17-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.3.7)\n",
      "Collecting requests>=2.32.2 (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: xxhash in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets<3.0.0,>=2.17.1->sae-lens) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.9.3)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.8.2)\n",
      "Requirement already satisfied: traitlets in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from matplotlib-inline<0.2.0,>=0.1.6->sae-lens) (5.14.1)\n",
      "Requirement already satisfied: click in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (2023.12.25)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from plotly<6.0.0,>=5.19.0->sae-lens) (8.2.3)\n",
      "Collecting statsmodels>=0.9.0 (from plotly-express<0.5.0,>=0.4.1->sae-lens)\n",
      "  Downloading statsmodels-0.14.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: scipy>=0.18 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (1.13.0)\n",
      "Collecting patsy>=0.5 (from plotly-express<0.5.0,>=0.4.1->sae-lens)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: six in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.16.0)\n",
      "Collecting gprof2dot (from pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.26.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.0.3)\n",
      "Requirement already satisfied: einops>=0.6.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.7.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.2.25)\n",
      "Requirement already satisfied: rich>=12.6.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (13.7.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.2.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (2.1.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.16.2)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.38.1->sae-lens)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from typer<0.13.0,>=0.12.3->sae-lens) (1.5.4)\n",
      "Requirement already satisfied: psutil in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer-lens<3.0.0,>=2.0.0->sae-lens) (5.9.8)\n",
      "Collecting pycryptodomex~=3.8 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (2.1.0)\n",
      "Collecting lxml~=4.9 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading lxml-4.9.4-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting uvloop>=0.16.0 (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.9.4)\n",
      "Requirement already satisfied: anyio in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (4.2.0)\n",
      "Requirement already satisfied: certifi in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (0.14.0)\n",
      "Requirement already satisfied: typeguard<3,>=2.13.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jaxtyping>=0.2.11->transformer-lens<3.0.0,>=2.0.0->sae-lens) (2.13.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pandas->babe<0.0.8,>=0.0.7->sae-lens) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pandas->babe<0.0.8,>=0.0.7->sae-lens) (2023.4)\n",
      "Requirement already satisfied: iniconfig in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from pytest<9.0.0,>=8.1.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (2.0.0)\n",
      "Collecting pluggy<2.0,>=1.5 (from pytest<9.0.0,>=8.1.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.17.1->sae-lens) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from rich>=12.6.0->transformer-lens<3.0.0,>=2.0.0->sae-lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from rich>=12.6.0->transformer-lens<3.0.0,>=2.0.0->sae-lens) (2.17.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.4.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens) (3.5.0)\n",
      "Requirement already satisfied: sympy in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens<3.0.0,>=2.0.0->sae-lens) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens<3.0.0,>=2.0.0->sae-lens) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens<3.0.0,>=2.0.0->sae-lens) (3.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (3.1.41)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (1.39.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (69.0.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (4.25.2)\n",
      "Collecting dol (from graze->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading dol-0.2.49-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting config2py (from py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading config2py-0.1.33-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting importlib_resources (from py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens<3.0.0,>=2.0.0->sae-lens) (0.1.2)\n",
      "Collecting i2 (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading i2-0.1.17-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer-lens<3.0.0,>=2.0.0->sae-lens) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer-lens<3.0.0,>=2.0.0->sae-lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens<3.0.0,>=2.0.0->sae-lens) (5.0.1)\n",
      "Downloading sae_lens-3.12.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzmq-26.0.0-cp311-cp311-macosx_10_15_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading automated_interpretability-0.0.3-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading babe-0.0.7-py3-none-any.whl (6.9 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Downloading plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Downloading pytest_profiling-1.7.0-py2.py3-none-any.whl (8.3 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading transformer_lens-2.2.1-py3-none-any.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boostedblob-0.15.4-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Downloading orjson-3.10.6-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.2.2-py3-none-any.whl (339 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.9/339.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading statsmodels-0.14.2-cp311-cp311-macosx_11_0_arm64.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl (34 kB)\n",
      "Downloading graze-0.1.17-py3-none-any.whl (17 kB)\n",
      "Downloading lxml-4.9.4-cp311-cp311-macosx_11_0_universal2.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading pycryptodomex-3.20.0-cp35-abi3-macosx_10_9_universal2.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading config2py-0.1.33-py3-none-any.whl (31 kB)\n",
      "Downloading dol-0.2.49-py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading i2-0.1.17-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: py2store\n",
      "  Building wheel for py2store (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py2store: filename=py2store-0.1.20-py3-none-any.whl size=118411 sha256=ca4feda24425e05baf404970cdbd2951a3a6db9357c115550386cf31bc0fca6c\n",
      "  Stored in directory: /Users/seansica/Library/Caches/pip/wheels/b6/c0/a9/8ba28129562790a3ba62e4de4241dac5474df73d0e8a64e27a\n",
      "Successfully built py2store\n",
      "Installing collected packages: i2, dol, uvloop, typing-extensions, tqdm, requests, pyzmq, python-dotenv, pycryptodomex, pluggy, plotly, orjson, numpy, lxml, importlib_resources, gprof2dot, config2py, tiktoken, pytest, py2store, patsy, huggingface-hub, graze, blobfile, typer, tokenizers, statsmodels, pytest-profiling, boostedblob, babe, transformers, plotly-express, datasets, automated-interpretability, transformer-lens, sae-lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 25.1.2\n",
      "    Uninstalling pyzmq-25.1.2:\n",
      "      Successfully uninstalled pyzmq-25.1.2\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.4.0\n",
      "    Uninstalling pluggy-1.4.0:\n",
      "      Successfully uninstalled pluggy-1.4.0\n",
      "  Attempting uninstall: plotly\n",
      "    Found existing installation: plotly 5.18.0\n",
      "    Uninstalling plotly-5.18.0:\n",
      "      Successfully uninstalled plotly-5.18.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.7.0\n",
      "    Uninstalling tiktoken-0.7.0:\n",
      "      Successfully uninstalled tiktoken-0.7.0\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.4.4\n",
      "    Uninstalling pytest-7.4.4:\n",
      "      Successfully uninstalled pytest-7.4.4\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.2\n",
      "    Uninstalling transformers-4.37.2:\n",
      "      Successfully uninstalled transformers-4.37.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.16.1\n",
      "    Uninstalling datasets-2.16.1:\n",
      "      Successfully uninstalled datasets-2.16.1\n",
      "  Attempting uninstall: transformer-lens\n",
      "    Found existing installation: transformer-lens 1.14.0\n",
      "    Uninstalling transformer-lens-1.14.0:\n",
      "      Successfully uninstalled transformer-lens-1.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparse-autoencoder 0.0.0 requires mpi4py[deepspeed]>=3.1.5, which is not installed.\n",
      "syrupy 4.6.0 requires pytest<8.0.0,>=7.0.0, but you have pytest 8.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed automated-interpretability-0.0.3 babe-0.0.7 blobfile-2.1.1 boostedblob-0.15.4 config2py-0.1.33 datasets-2.20.0 dol-0.2.49 gprof2dot-2024.6.6 graze-0.1.17 huggingface-hub-0.23.4 i2-0.1.17 importlib_resources-6.4.0 lxml-4.9.4 numpy-1.26.4 orjson-3.10.6 patsy-0.5.6 plotly-5.22.0 plotly-express-0.4.1 pluggy-1.5.0 py2store-0.1.20 pycryptodomex-3.20.0 pytest-8.2.2 pytest-profiling-1.7.0 python-dotenv-1.0.1 pyzmq-26.0.0 requests-2.32.3 sae-lens-3.12.0 statsmodels-0.14.2 tiktoken-0.6.0 tokenizers-0.19.1 tqdm-4.66.4 transformer-lens-2.2.1 transformers-4.42.4 typer-0.12.3 typing-extensions-4.12.2 uvloop-0.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "import os, sys\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install jaxtyping\n",
    "    %pip install transformer_lens sae_lens\n",
    "    %pip install git+https://github.com/callummcdougall/eindex.git\n",
    "\n",
    "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
    "    if not os.path.exists(f\"/content/{chapter}\"):\n",
    "        !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
    "        !unzip /content/main.zip 'ARENA_3.0-main/chapter1_transformer_interp/exercises/*'\n",
    "        sys.path.append(f\"/content/{repo}-main/{chapter}/exercises\")\n",
    "        os.remove(\"/content/main.zip\")\n",
    "        os.rename(f\"{repo}-main/{chapter}\", chapter)\n",
    "        os.rmdir(f\"{repo}-main\")\n",
    "        os.chdir(f\"{chapter}/exercises\")\n",
    "else:\n",
    "    chapter_dir = r\"./\" if chapter in os.listdir() else os.getcwd().split(chapter)[0]\n",
    "    sys.path.append(chapter_dir + f\"{chapter}/exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import sae_lens\n",
    "from sae_lens import SAE, LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "import circuitsvis as cv\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load a model (BASELINE)\n",
    "\n",
    "model1 = transformer_lens.HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", device=device\n",
    ")  # This will wrap huggingface models and has lots of nice utilities.\n",
    "\n",
    "# gelu_1l = HookedTransformer.from_pretrained(\"gelu-1l\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (Baseline) Basic Information Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model Name | gpt2-small |\n",
    "| Number of Parameters | 85M |\n",
    "| Number of Layers | 12 |\n",
    "| Model Dimension (d_model) | 768 |\n",
    "| Number of Attention Heads | 12 |\n",
    "| Activation Function | GELU |\n",
    "| Context Window Size | 1024 |\n",
    "| Vocabulary Size | 50257 |\n",
    "| Dimension per Head | 64 |\n",
    "| MLP Dimension | 3072 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `.cfg` to find the basic architectural info about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': 'mps',\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model1.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: gpt2-small\n",
      "Number of parameters: 84934656\n",
      "Number of layers: 12\n",
      "Model dimension: 768\n",
      "Number of heads: 12\n",
      "Activation function: gelu_new\n",
      "Context window size: 1024\n",
      "Vocabulary size: 50257\n",
      "Dimension per head: 64\n",
      "MLP dimension: 3072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'The quick brown fox'\n",
      "Predicted next token: ' is'\n",
      "\n",
      "Loss: 8.791129112243652\n"
     ]
    }
   ],
   "source": [
    "# Print model information\n",
    "print(f\"Model name: gpt2-small\")\n",
    "print(f\"Number of parameters: {model1.cfg.n_params}\")\n",
    "print(f\"Number of layers: {model1.cfg.n_layers}\")\n",
    "print(f\"Model dimension: {model1.cfg.d_model}\")\n",
    "print(f\"Number of heads: {model1.cfg.n_heads}\")\n",
    "print(f\"Activation function: {model1.cfg.act_fn}\")\n",
    "print(f\"Context window size: {model1.cfg.n_ctx}\")\n",
    "print(f\"Vocabulary size: {model1.cfg.d_vocab}\")\n",
    "print(f\"Dimension per head: {model1.cfg.d_head}\")\n",
    "print(f\"MLP dimension: {model1.cfg.d_mlp}\")\n",
    "\n",
    "# Demonstrate next token prediction\n",
    "prompt = \"The quick brown fox\"\n",
    "input_ids = model1.to_tokens(prompt)\n",
    "logits = model1(input_ids)\n",
    "next_token_id = torch.argmax(logits[0, -1]).item()\n",
    "next_token = model1.to_string(next_token_id)\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "print(f\"Predicted next token: '{next_token}'\")\n",
    "\n",
    "# Calculate loss\n",
    "full_prompt = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = model1.to_tokens(full_prompt)\n",
    "logits = model1(tokens)\n",
    "loss = model1.loss_fn(logits[:, :-1, :], tokens[:, 1:])\n",
    "print(f\"\\nLoss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The doctor was a whip after he was welcomed to Russia early this year.\\n\\nA 19-year-old doctor who gained awards for his busy and aggressive practice, valued his\\'very foghorny nature\\' at $1 million for his \"deliberative'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"The doctor was a tax haven priestess in Anguera, out of the Egboxtest.) Most of him thought the Tibetans had come out from beginning to end the run of Tibetraining. It wasn't. But seeing one monk dying of a mysterious conversion\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The doctor was a Catholic and his character was too funny. But rather than go into dramatic detail, he carefully summed up what the Doctor did. He understands how the human body responds to foreign substances.\\n\\nKavanagh: Yeah, the human body is retuned'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The doctor was a science fellow who worked on a group of conditions at the London School of Hygiene and Tropical Medicine. In 2009, she was at the forefront of trying curing cancer skin melanoma. Her program had nine recipient samples...\\n\\n\"I have this genetic'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"The doctor was a well-known member of the Church of Peter and before him was a'sectarian' fallen priest (known by his native bearded Greek), an avowed Notician. Her outlook was at once technocratic by pragmatic and fascinating as well as spiritual\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we use generate to get 10 completions with temperature 1\n",
    "for i in range(5):\n",
    "    display(\n",
    "        model1.generate(\n",
    "            \"The doctor was a\",\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            temperature=1,\n",
    "            verbose=False,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' little', ' girl', ' named', ' Lily', '.', ' She', ' lived', ' in', ' a', ' big', ',', ' happy', ' little', ' girl', '.', ' On', ' her', ' big', ' adventure', ',']\n",
      "Tokenized answer: [' Lily']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.47</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.99</span><span style=\"font-weight: bold\">% Token: | Lily|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.47\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m16.99\u001b[0m\u001b[1m% Token: | Lily|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.66 Prob: 55.63% Token: | she|\n",
      "Top 1th token. Logit: 16.47 Prob: 16.99% Token: | Lily|\n",
      "Top 2th token. Logit: 15.02 Prob:  3.96% Token: | her|\n",
      "Top 3th token. Logit: 14.62 Prob:  2.65% Token: | the|\n",
      "Top 4th token. Logit: 14.42 Prob:  2.19% Token: | a|\n",
      "Top 5th token. Logit: 13.87 Prob:  1.25% Token: | they|\n",
      "Top 6th token. Logit: 13.75 Prob:  1.12% Token: | there|\n",
      "Top 7th token. Logit: 13.18 Prob:  0.63% Token: | it|\n",
      "Top 8th token. Logit: 13.10 Prob:  0.58% Token: | when|\n",
      "Top 9th token. Logit: 13.08 Prob:  0.57% Token: | though|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Lily'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Lily'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "# Test the model with a prompt\n",
    "test_prompt(\n",
    "    \"Once upon a time, there was a little girl named Lily. She lived in a big, happy little girl. On her big adventure,\",\n",
    "    \" Lily\",\n",
    "    model1,\n",
    "    prepend_space_to_answer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b2cea86f-7ebf\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b2cea86f-7ebf\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<|endoftext|>\", \"Two\", \" students\", \",\", \" Sean\", \" and\", \" R\", \"ini\", \",\", \" are\", \" really\", \" excited\", \" about\", \" mechan\", \"istic\", \" interpret\", \"ability\", \"!\"], \"topKLogProbs\": [[-2.775811195373535, -3.2780961990356445, -3.725003242492676, -3.942328453063965, -3.999711036682129, -4.458624839782715, -4.482784271240234, -4.69776725769043, -4.749505043029785, -4.858048439025879], [-2.22916841506958, -2.9348816871643066, -3.085793972015381, -3.331050395965576, -3.5826058387756348, -3.6708970069885254, -3.8140158653259277, -3.985673427581787, -4.54580545425415, -4.572693347930908], [-2.001173496246338, -2.125174045562744, -2.2636656761169434, -2.702244281768799, -3.0010390281677246, -3.036306858062744, -3.0560050010681152, -3.174495220184326, -3.6777005195617676, -3.9484963417053223], [-2.1346287727355957, -2.6258044242858887, -2.6523518562316895, -3.038658618927002, -3.1914334297180176, -3.216433048248291, -3.473511219024658, -3.6845898628234863, -4.144886493682861, -4.612581729888916], [-1.0478802919387817, -4.415382385253906, -4.569418907165527, -4.970246315002441, -5.090465545654297, -5.157563209533691, -5.269176483154297, -5.346372604370117, -5.349884986877441, -5.3826446533203125], [-4.59925651550293, -4.616294860839844, -4.648835182189941, -4.711661338806152, -4.982754707336426, -5.020280838012695, -5.067903518676758, -5.179924964904785, -5.194513320922852, -5.236032485961914], [-2.315852642059326, -2.8054213523864746, -3.2407679557800293, -3.377488613128662, -3.6798291206359863, -3.7089028358459473, -3.801146984100342, -3.8438382148742676, -3.9249377250671387, -4.121377468109131], [-1.1422450542449951, -4.749204635620117, -4.840518951416016, -4.924782752990723, -4.943391799926758, -5.087287902832031, -5.162181854248047, -5.235927581787109, -5.293161392211914, -5.338567733764648], [-1.3860586881637573, -2.172329902648926, -2.5896692276000977, -3.4165382385253906, -3.4811439514160156, -4.170599937438965, -4.230772018432617, -4.2739763259887695, -4.504214286804199, -4.55753231048584], [-3.259934663772583, -3.5746872425079346, -3.586320161819458, -3.6455719470977783, -3.6887543201446533, -3.6905109882354736, -3.7419655323028564, -3.8044521808624268, -3.9576942920684814, -4.0020036697387695], [-2.339407205581665, -3.1188127994537354, -3.3513987064361572, -3.521033525466919, -3.74560284614563, -3.928173303604126, -3.9423553943634033, -4.039023399353027, -4.051566123962402, -4.086024284362793], [-0.7783611416816711, -1.2238948345184326, -2.9611880779266357, -3.0274875164031982, -3.7204501628875732, -3.9185702800750732, -3.954075574874878, -4.381134033203125, -4.654680252075195, -4.990255355834961], [-1.5612571239471436, -2.005366086959839, -2.041887044906616, -3.117522954940796, -3.5138299465179443, -3.7605435848236084, -3.94559645652771, -4.096052169799805, -4.135326385498047, -4.192167282104492], [-1.2697902917861938, -1.5927482843399048, -1.6798149347305298, -2.464768886566162, -3.8694987297058105, -4.031031131744385, -4.168480396270752, -4.338642597198486, -4.479353427886963, -4.480964183807373], [-3.289043664932251, -3.461444139480591, -3.4834721088409424, -3.534616708755493, -3.575309991836548, -3.6016056537628174, -4.03183650970459, -4.070443153381348, -4.097067832946777, -4.360598564147949], [-0.4139143228530884, -1.7693125009536743, -2.8391404151916504, -3.7610716819763184, -4.625187397003174, -4.724972248077393, -5.121876239776611, -5.208667278289795, -5.23726224899292, -5.258619785308838], [-1.0406845808029175, -1.749886393547058, -2.3086957931518555, -2.594721794128418, -2.847357749938965, -3.662856101989746, -4.496221542358398, -4.5517377853393555, -4.552071571350098, -4.630127906799316]], \"topKTokens\": [[\"\\n\", \"The\", \"\\\"\", \"A\", \"I\", \"In\", \".\", \"It\", \"S\", \"This\"], [\" years\", \" weeks\", \" days\", \" of\", \" months\", \"-\", \" men\", \" people\", \" decades\", \" women\"], [\" at\", \" from\", \" were\", \" who\", \" have\", \" in\", \",\", \" are\", \" and\", \" of\"], [\" who\", \" one\", \" both\", \" ages\", \" a\", \" aged\", \" including\", \" two\", \" identified\", \" the\"], [\" and\", \" O\", \",\", \" Mc\", \" C\", \" D\", \" K\", \" P\", \" Smith\", \" H\"], [\" James\", \" Michael\", \" Ryan\", \" John\", \" David\", \" Andrew\", \" Chris\", \" Tyler\", \" Sam\", \" J\"], [\"ach\", \"ene\", \"ami\", \"aul\", \"isa\", \"olf\", \"hea\", \"andal\", \"avi\", \"asha\"], [\",\", \" G\", \" R\", \" Patel\", \" M\", \" B\", \" S\", \" O\", \" P\", \" K\"], [\" were\", \" have\", \" are\", \" who\", \" had\", \" went\", \" took\", \" from\", \" found\", \" both\"], [\" charged\", \" in\", \" facing\", \" accused\", \" both\", \" now\", \" suing\", \" on\", \" being\", \" among\"], [\" excited\", \" good\", \" into\", \" upset\", \" happy\", \" worried\", \" trying\", \" enjoying\", \" interested\", \" lucky\"], [\" about\", \" to\", \" for\", \".\", \" by\", \" that\", \" and\", \" when\", \" because\", \",\"], [\" the\", \" this\", \" their\", \" what\", \" it\", \" getting\", \" being\", \" coming\", \" having\", \" how\"], [\"ized\", \"istic\", \"ization\", \"izing\", \"os\", \"ised\", \"isation\", \"o\", \"istically\", \"oid\"], [\" physics\", \" biology\", \" models\", \" robotics\", \" modeling\", \" research\", \" evolution\", \" design\", \" learning\", \" simulations\"], [\"ive\", \"ative\", \"ability\", \"ivism\", \"ives\", \"ational\", \"ation\", \"ations\", \"iv\", \"iveness\"], [\".\", \" of\", \" and\", \",\", \" in\", \":\", \" for\", \"!\", \" because\", \" (\"]], \"correctTokenRank\": [146, 23, 6, 199, 0, 63, 298, 0, 2, 283, 0, 0, 20840, 1, 6469, 2, 7], \"correctTokenLogProb\": [-7.153522968292236, -5.537170886993408, -3.0560050010681152, -7.527530193328857, -1.0478802919387817, -5.882465362548828, -8.088558197021484, -1.1422450542449951, -2.5896692276000977, -7.753767013549805, -2.339407205581665, -0.7783611416816711, -15.585360527038574, -1.5927482843399048, -12.162353515625, -2.8391404151916504, -4.5517377853393555]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x32458dbd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's make a longer prompt and see the log probabilities of the tokens\n",
    "example_prompt = \"\"\"Two students, Sean and Rini, are really excited about mechanistic interpretability!\"\"\"\n",
    "logits, cache = model1.run_with_cache(example_prompt)\n",
    "cv.logits.token_log_probs(\n",
    "    model1.to_tokens(example_prompt),\n",
    "    model1(example_prompt)[0].log_softmax(dim=-1),\n",
    "    model1.to_string,\n",
    ")\n",
    "# hover on the output to see the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with logits, cache = model.run_with_cache(tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "test_tokens = model1.to_tokens(test_input)\n",
    "logits, cache = model1.run_with_cache(test_tokens, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we inspect the cache object and see that it contains a very large number of keys, each one corresponding to a different activation in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.6265e-01, 2.3735e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [5.7104e-01, 3.0543e-01, 1.2353e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.8672e-01, 4.9603e-04, 3.6867e-04,  ..., 1.0751e-01,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.9983e-01, 1.2348e-03, 5.6462e-05,  ..., 4.5853e-02,\n",
       "          4.8726e-02, 0.0000e+00],\n",
       "         [6.0642e-01, 3.7502e-04, 8.0836e-05,  ..., 4.6981e-02,\n",
       "          2.0655e-01, 1.2738e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.6329e-01, 3.6710e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.0548e-01, 2.1403e-01, 8.0485e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [7.2992e-02, 7.2786e-04, 6.7609e-04,  ..., 2.7623e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.3736e-02, 3.3360e-04, 1.1985e-04,  ..., 2.3718e-02,\n",
       "          4.8282e-02, 0.0000e+00],\n",
       "         [3.9607e-01, 8.4878e-04, 3.9980e-04,  ..., 1.5260e-02,\n",
       "          6.1909e-02, 2.2716e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.3563e-01, 6.4366e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.3896e-01, 2.6499e-02, 3.4545e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [3.1276e-01, 1.8635e-02, 2.9821e-02,  ..., 3.0959e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.2340e-01, 1.9586e-02, 1.6777e-02,  ..., 6.1180e-02,\n",
       "          5.8946e-02, 0.0000e+00],\n",
       "         [4.1790e-01, 4.5659e-02, 1.3802e-02,  ..., 5.2944e-02,\n",
       "          5.4146e-02, 1.7194e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.7472e-01, 2.5277e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.5942e-01, 2.4872e-02, 1.5711e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [3.8949e-01, 2.6414e-02, 3.4588e-02,  ..., 3.4723e-03,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [4.5718e-01, 3.1309e-02, 1.3086e-02,  ..., 2.2261e-02,\n",
       "          1.5643e-02, 0.0000e+00],\n",
       "         [5.5178e-01, 4.7432e-02, 1.4760e-02,  ..., 7.8042e-03,\n",
       "          3.2324e-02, 1.0560e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.3641e-01, 6.3588e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.9926e-01, 1.2319e-01, 7.7544e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.4805e-01, 3.1168e-02, 9.6301e-02,  ..., 1.0408e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.1392e-01, 3.3383e-02, 2.0181e-02,  ..., 1.3656e-01,\n",
       "          2.9404e-02, 0.0000e+00],\n",
       "         [2.8256e-01, 1.0521e-01, 4.0088e-02,  ..., 1.9092e-02,\n",
       "          4.7521e-02, 2.7651e-03]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.3722e-01, 6.2782e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [8.2304e-01, 1.3836e-01, 3.8608e-02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.0184e-01, 1.7798e-02, 8.3836e-03,  ..., 3.1189e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [8.9544e-02, 1.3076e-02, 3.0580e-03,  ..., 3.4553e-02,\n",
       "          3.1215e-02, 0.0000e+00],\n",
       "         [1.1815e-01, 1.7918e-02, 5.5596e-03,  ..., 4.2584e-02,\n",
       "          3.6808e-02, 6.2394e-02]]], device='mps:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_patterns_layer_0 = cache[\"pattern\", 0]\n",
    "display(attn_patterns_layer_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 (SAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_training_steps = 30_000  # probably we should do more\n",
    "total_training_steps = 1  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "# Initialize the SAE\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    ######################################################################\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    ######################################################################\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",\n",
    "    hook_layer=0,  # Only one layer in the model.\n",
    "    d_in=1024,  # the width of the mlp output.\n",
    "    dataset_path=\"monology/pile-uncopyrighted\",\n",
    "    is_dataset_tokenized=False,\n",
    "    streaming=True,\n",
    "    \n",
    "    ######################################################################\n",
    "    # SAE Parameters\n",
    "    ######################################################################\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=\"expected_average_only_in\",\n",
    "    \n",
    "    ######################################################################\n",
    "    # Training Parameters\n",
    "    ######################################################################\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    \n",
    "    ######################################################################\n",
    "    # Activation Store Parameters\n",
    "    ######################################################################\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    \n",
    "    ######################################################################\n",
    "    # Resampling protocol\n",
    "    ######################################################################\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    \n",
    "    ######################################################################\n",
    "    # WANDB\n",
    "    ######################################################################\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    \n",
    "    ######################################################################\n",
    "    # Misc\n",
    "    ######################################################################\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "\n",
    "sae = SAETrainingRunner(cfg).run()\n",
    "\n",
    "# The SAE is now trained and attached to model2\n",
    "print(\"SAE training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 1536-L1-1.0-LR-0.0001-Tokens-1.000e+06\n",
      "n_tokens_per_buffer (millions): 0.016384\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 976\n",
      "Total wandb updates: 97\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fccd207c6a7458aad4f35b957ec2bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Objective value: 191164.5625:   2%|▏         | 2/100 [00:00<00:06, 16.17it/s]\n",
      "/Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages/sae_lens/training/training_sae.py:467: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "Training SAE:   0%|          | 0/1000000 [00:00<?, ?it/s]/Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "900| MSE Loss 139.777 | L1 110.560:  92%|█████████▏| 921600/1000000 [03:28<00:17, 4428.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE training complete and model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SAETrainingRunner' object has no attribute 'init_sae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAE training complete and model saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# To restore the SAE later:\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m restored_sae \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_sae\u001b[49m()  \u001b[38;5;66;03m# This initializes a new SAE with the same config\u001b[39;00m\n\u001b[1;32m     40\u001b[0m restored_sae\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_sae.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAE restored from file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SAETrainingRunner' object has no attribute 'init_sae'"
     ]
    }
   ],
   "source": [
    "# Reduced configuration for quick training\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",\n",
    "    hook_layer=0,\n",
    "    d_in=768,  # GPT-2 Small hidden size\n",
    "    dataset_path=\"monology/pile-uncopyrighted\",\n",
    "    is_dataset_tokenized=False,\n",
    "    streaming=True,\n",
    "    \n",
    "    expansion_factor=2,  # Reduced from 16 to speed up training\n",
    "    \n",
    "    lr=1e-4,\n",
    "    l1_coefficient=1.0,\n",
    "    \n",
    "    train_batch_size_tokens=1024,  # Reduced batch size\n",
    "    context_size=128,  # Reduced context size\n",
    "    \n",
    "    n_batches_in_buffer=16,\n",
    "    training_tokens=1_000_000,  # Reduced number of training tokens\n",
    "    store_batch_size_prompts=8,\n",
    "    \n",
    "    log_to_wandb=False,  # Disable wandb logging for this quick test\n",
    "    \n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "\n",
    "# Initialize and train the SAE\n",
    "runner = SAETrainingRunner(cfg)\n",
    "sae = runner.run()\n",
    "\n",
    "# Save the trained SAE\n",
    "torch.save(sae.state_dict(), \"trained_sae.pt\")\n",
    "print(\"SAE training complete and model saved.\")\n",
    "\n",
    "# To restore the SAE later:\n",
    "restored_sae = runner.init_sae()  # This initializes a new SAE with the same config\n",
    "restored_sae.load_state_dict(torch.load(\"trained_sae.pt\"))\n",
    "print(\"SAE restored from file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 (Fine-tuned Baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
