{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HookedTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, num_layers):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, activation=\"relu\"\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.mlp_activations = []\n",
    "\n",
    "    def forward(self, src):\n",
    "        return self.transformer(src)\n",
    "\n",
    "    def get_mlp_activations(self, module, input, output):\n",
    "        print(f\"Hook called for module: {module}\")\n",
    "        self.mlp_activations.append(output.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hooked_transformer():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Initialize the transformer\n",
    "    d_model = 64\n",
    "    nhead = 1\n",
    "    dim_feedforward = 512\n",
    "    num_layers = 1\n",
    "    transformer = HookedTransformer(d_model, nhead, dim_feedforward, num_layers)\n",
    "\n",
    "    # Register the hook\n",
    "    hooks = []\n",
    "    for name, module in transformer.named_modules():\n",
    "        if (\n",
    "            \"linear2\" in name\n",
    "        ):  # This is the second linear layer in the feed-forward network\n",
    "            print(f\"Registering forward hook for module: {name}\")\n",
    "            hook = module.register_forward_hook(transformer.get_mlp_activations)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Create input tensor\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    input_tensor = torch.randn(seq_length, batch_size, d_model)\n",
    "\n",
    "    # Forward pass\n",
    "    output = transformer(input_tensor)\n",
    "\n",
    "    # Check if MLP activations were collected\n",
    "    assert len(transformer.mlp_activations) > 0, \"No MLP activations were collected\"\n",
    "\n",
    "    # Check the shape of collected activations\n",
    "    expected_shape = (seq_length, batch_size, d_model)  # Corrected shape\n",
    "    actual_shape = transformer.mlp_activations[0].shape\n",
    "    assert (\n",
    "        actual_shape == expected_shape\n",
    "    ), f\"Expected shape {expected_shape}, but got {actual_shape}\"\n",
    "\n",
    "    # We can't check for non-negative values as we're capturing before ReLU\n",
    "    # But we can check if the activations are not all zero\n",
    "    assert not torch.all(\n",
    "        transformer.mlp_activations[0] == 0\n",
    "    ), \"All MLP activations are zero, which is unlikely\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")\n",
    "\n",
    "    # Remove the hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering forward hook for module: transformer.layers.0.linear2\n",
      "Hook called for module: Linear(in_features=512, out_features=64, bias=True)\n",
      "All tests passed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seansica/.pyenv/versions/3.11.9/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "test_hooked_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : HookedTransformer(\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "transformer : TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "transformer.layers : ModuleList(\n",
      "  (0): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "transformer.layers.0 : TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "transformer.layers.0.self_attn : MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      ")\n",
      "transformer.layers.0.self_attn.out_proj : NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "transformer.layers.0.linear1 : Linear(in_features=64, out_features=512, bias=True)\n",
      "transformer.layers.0.dropout : Dropout(p=0.1, inplace=False)\n",
      "transformer.layers.0.linear2 : Linear(in_features=512, out_features=64, bias=True)\n",
      "transformer.layers.0.norm1 : LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "transformer.layers.0.norm2 : LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "transformer.layers.0.dropout1 : Dropout(p=0.1, inplace=False)\n",
      "transformer.layers.0.dropout2 : Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "nhead = 1\n",
    "dim_feedforward = 512\n",
    "num_layers = 1\n",
    "transformer = HookedTransformer(d_model, nhead, dim_feedforward, num_layers)\n",
    "\n",
    "for name, module in transformer.named_modules():\n",
    "    print(f'{name} : {module}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(TransformerEncoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        self.activation_fn = nn.ReLU()  # Explicitly define ReLU as a module\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation_fn(x)  # Use the module version of ReLU\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class HookedTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, num_layers):\n",
    "        super().__init__()\n",
    "        encoder_layer = CustomTransformerEncoderLayer(d_model, nhead, dim_feedforward, activation='relu')\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.mlp_activations = []\n",
    "\n",
    "    def forward(self, src):\n",
    "        return self.transformer(src)\n",
    "\n",
    "    def get_mlp_activations(self, module, input, output):\n",
    "        print(f\"Hook called for module: {module}\")\n",
    "        self.mlp_activations.append(output.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering forward hook for module: transformer.layers.0.activation_fn\n",
      "Hook called for module: ReLU()\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_hooked_transformer():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Initialize the transformer\n",
    "    d_model = 64\n",
    "    nhead = 4\n",
    "    dim_feedforward = 512\n",
    "    num_layers = 1\n",
    "    transformer = HookedTransformer(d_model, nhead, dim_feedforward, num_layers)\n",
    "\n",
    "    # Register the hook\n",
    "    hooks = []\n",
    "    for name, module in transformer.named_modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            print(f\"Registering forward hook for module: {name}\")\n",
    "            hook = module.register_forward_hook(transformer.get_mlp_activations)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Create input tensor\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    input_tensor = torch.randn(seq_length, batch_size, d_model)\n",
    "\n",
    "    # Forward pass\n",
    "    output = transformer(input_tensor)\n",
    "\n",
    "    # Check if MLP activations were collected\n",
    "    assert len(transformer.mlp_activations) > 0, \"No MLP activations were collected\"\n",
    "\n",
    "    # Check the shape of collected activations\n",
    "    expected_shape = (seq_length, batch_size, dim_feedforward)\n",
    "    actual_shape = transformer.mlp_activations[0].shape\n",
    "    assert actual_shape == expected_shape, f\"Expected shape {expected_shape}, but got {actual_shape}\"\n",
    "\n",
    "    # Check if activations are non-negative (due to ReLU)\n",
    "    assert torch.all(transformer.mlp_activations[0] >= 0), \"ReLU activations should be non-negative\"\n",
    "\n",
    "    # Check if some activations are positive (not all zeros)\n",
    "    assert torch.any(transformer.mlp_activations[0] > 0), \"All ReLU activations are zero, which is unlikely\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")\n",
    "\n",
    "    # Remove the hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_hooked_transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
